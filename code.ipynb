{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c1eff4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T15:47:26.636140Z",
     "iopub.status.busy": "2025-03-25T15:47:26.634931Z",
     "iopub.status.idle": "2025-03-25T15:47:27.020201Z",
     "shell.execute_reply": "2025-03-25T15:47:27.019360Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ML Components Implementation Requirements for Period Tracker App\n",
      "\n",
      "## 1. FastAPI Application Structure\n",
      "- Create core FastAPI application with necessary middlewares\n",
      "- Set up routing for prediction and training endpoints\n",
      "- Implement authentication for API access from Laravel\n",
      "- Configure CORS and security settings\n",
      "\n",
      "## 2. Data Models and Processing Pipeline\n",
      "- Implement preprocessing for various data types (cycle, symptoms, biometrics)\n",
      "- Create feature engineering pipeline for derived metrics\n",
      "- Develop data validation with Pydantic models\n",
      "- Implement anonymization functions for training data\n",
      "\n",
      "## 3. Machine Learning Models\n",
      "### 3.1 ARIMA Model\n",
      "- Focus: Baseline cycle prediction using time series analysis\n",
      "- Features: Historical cycle lengths and patterns\n",
      "- Target accuracy: ~87% for regular cycles\n",
      "\n",
      "### 3.2 Random Forest Regressor\n",
      "- Focus: Symptom impact on cycle timing\n",
      "- Features: Symptoms, lifestyle factors, historical patterns\n",
      "- Target accuracy: ~85% for symptom-cycle correlations\n",
      "\n",
      "### 3.3 Gradient Boosting Regressor\n",
      "- Focus: Fertility window and ovulation timing\n",
      "- Features: Temperature, fertility indicators, symptoms\n",
      "- Target accuracy: ~82% for fertility predictions\n",
      "\n",
      "### 3.4 Ensemble Integrator\n",
      "- Weighted averaging of individual model predictions\n",
      "- Dynamic weight adjustment based on user-specific performance\n",
      "- Confidence score calculation based on model agreement\n",
      "\n",
      "## 4. MLflow Integration\n",
      "- Set up experiment tracking for model training\n",
      "- Implement model versioning and registry\n",
      "- Automate model deployment and rollback processes\n",
      "- Track prediction accuracy metrics over time\n",
      "\n",
      "## 5. Continuous Learning Pipeline\n",
      "- Create scheduled retraining jobs (weekly global, monthly user-specific)\n",
      "- Implement user feedback collection and integration\n",
      "- Develop model evaluation and comparison framework\n",
      "- Set up model drift detection and alerts\n",
      "\n",
      "## 6. API Endpoints\n",
      "- Period prediction endpoints\n",
      "- Fertility and ovulation prediction endpoints\n",
      "- Training initiation endpoints\n",
      "- Model management endpoints (list, details, metrics)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Analyze the ML requirements from the system architecture documents\n",
    "def analyze_ml_requirements():\n",
    "    \"\"\"\n",
    "    Analyze the ML requirements from the architecture documents and \n",
    "    summarize the key components that need to be implemented.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Summary of ML components based on the system architecture documents\n",
    "    ml_components = {\n",
    "        \"Model Components\": [\n",
    "            {\n",
    "                \"name\": \"ARIMA Time Series Model\",\n",
    "                \"purpose\": \"Baseline period prediction based on historical cycle lengths\",\n",
    "                \"implementation\": \"statsmodels ARIMA implementation\",\n",
    "                \"input_features\": [\"Historical cycle lengths\", \"Seasonal patterns\"],\n",
    "                \"expected_accuracy\": \"~87% for regular cycles\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Random Forest Regressor\",\n",
    "                \"purpose\": \"Evaluate symptom impact on cycle timing\",\n",
    "                \"implementation\": \"scikit-learn RandomForestRegressor\",\n",
    "                \"input_features\": [\"Symptoms\", \"Lifestyle factors\", \"Historical patterns\"],\n",
    "                \"expected_accuracy\": \"~85% for symptom-cycle correlations\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Gradient Boosting Regressor\",\n",
    "                \"purpose\": \"Fertility window and ovulation timing prediction\",\n",
    "                \"implementation\": \"scikit-learn GradientBoostingRegressor\",\n",
    "                \"input_features\": [\"Temperature data\", \"Fertility indicators\", \"Symptoms\"],\n",
    "                \"expected_accuracy\": \"~82% for fertility predictions\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Ensemble Integrator\",\n",
    "                \"purpose\": \"Combine predictions from individual models\",\n",
    "                \"implementation\": \"Weighted averaging with confidence scoring\",\n",
    "                \"logic\": \"Adjust weights based on model performance for each user\",\n",
    "                \"expected_combined_accuracy\": \"85-90%\"\n",
    "            }\n",
    "        ],\n",
    "        \n",
    "        \"Data Preprocessing\": [\n",
    "            \"Extract anonymized training data from PostgreSQL\",\n",
    "            \"Normalize features (scaling)\",\n",
    "            \"Impute missing data\",\n",
    "            \"Feature engineering for derived metrics\",\n",
    "            \"Outlier detection and handling\"\n",
    "        ],\n",
    "        \n",
    "        \"Training Pipeline\": [\n",
    "            \"Split data into training and validation sets\",\n",
    "            \"Global model training on all anonymized data\",\n",
    "            \"User-specific model fine-tuning for users with sufficient data\",\n",
    "            \"Hyperparameter optimization\",\n",
    "            \"Model validation on holdout data\",\n",
    "            \"Compare with previous model versions\",\n",
    "            \"A/B testing for new model versions\"\n",
    "        ],\n",
    "        \n",
    "        \"Continuous Learning\": [\n",
    "            \"Collect user corrections on predictions\",\n",
    "            \"Weight feedback based on recency and consistency\",\n",
    "            \"Weekly retraining of global models\",\n",
    "            \"Monthly retraining of user-specific models\",\n",
    "            \"On-demand retraining triggered by significant new data\",\n",
    "            \"Track prediction accuracy in production\",\n",
    "            \"Detect model drift or degradation\",\n",
    "            \"Alert on significant accuracy drops\"\n",
    "        ],\n",
    "        \n",
    "        \"FastAPI Implementation\": [\n",
    "            \"API endpoints for batch and real-time predictions\",\n",
    "            \"Versioned model endpoints for backward compatibility\",\n",
    "            \"Prediction confidence scores with every response\"\n",
    "        ],\n",
    "        \n",
    "        \"MLflow Integration\": [\n",
    "            \"Model versioning and tracking\",\n",
    "            \"Performance metrics tracking\",\n",
    "            \"Automated deployment and rollback\"\n",
    "        ],\n",
    "        \n",
    "        \"Key Performance Indicators\": [\n",
    "            \"Prediction Accuracy: 85-90%\",\n",
    "            \"Cycle Prediction Error Margin: <3 days\",\n",
    "            \"Data Processing Efficiency: <10 minutes for complete model retraining\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Print a structured summary of the ML requirements\n",
    "    print(\"# ML Components Implementation Requirements for Period Tracker App\")\n",
    "    print(\"\\n## 1. FastAPI Application Structure\")\n",
    "    print(\"- Create core FastAPI application with necessary middlewares\")\n",
    "    print(\"- Set up routing for prediction and training endpoints\")\n",
    "    print(\"- Implement authentication for API access from Laravel\")\n",
    "    print(\"- Configure CORS and security settings\")\n",
    "    \n",
    "    print(\"\\n## 2. Data Models and Processing Pipeline\")\n",
    "    print(\"- Implement preprocessing for various data types (cycle, symptoms, biometrics)\")\n",
    "    print(\"- Create feature engineering pipeline for derived metrics\")\n",
    "    print(\"- Develop data validation with Pydantic models\")\n",
    "    print(\"- Implement anonymization functions for training data\")\n",
    "    \n",
    "    print(\"\\n## 3. Machine Learning Models\")\n",
    "    print(\"### 3.1 ARIMA Model\")\n",
    "    print(\"- Focus: Baseline cycle prediction using time series analysis\")\n",
    "    print(\"- Features: Historical cycle lengths and patterns\")\n",
    "    print(\"- Target accuracy: ~87% for regular cycles\")\n",
    "    \n",
    "    print(\"\\n### 3.2 Random Forest Regressor\")\n",
    "    print(\"- Focus: Symptom impact on cycle timing\")\n",
    "    print(\"- Features: Symptoms, lifestyle factors, historical patterns\")\n",
    "    print(\"- Target accuracy: ~85% for symptom-cycle correlations\")\n",
    "    \n",
    "    print(\"\\n### 3.3 Gradient Boosting Regressor\")\n",
    "    print(\"- Focus: Fertility window and ovulation timing\")\n",
    "    print(\"- Features: Temperature, fertility indicators, symptoms\")\n",
    "    print(\"- Target accuracy: ~82% for fertility predictions\")\n",
    "    \n",
    "    print(\"\\n### 3.4 Ensemble Integrator\")\n",
    "    print(\"- Weighted averaging of individual model predictions\")\n",
    "    print(\"- Dynamic weight adjustment based on user-specific performance\")\n",
    "    print(\"- Confidence score calculation based on model agreement\")\n",
    "    \n",
    "    print(\"\\n## 4. MLflow Integration\")\n",
    "    print(\"- Set up experiment tracking for model training\")\n",
    "    print(\"- Implement model versioning and registry\")\n",
    "    print(\"- Automate model deployment and rollback processes\")\n",
    "    print(\"- Track prediction accuracy metrics over time\")\n",
    "    \n",
    "    print(\"\\n## 5. Continuous Learning Pipeline\")\n",
    "    print(\"- Create scheduled retraining jobs (weekly global, monthly user-specific)\")\n",
    "    print(\"- Implement user feedback collection and integration\")\n",
    "    print(\"- Develop model evaluation and comparison framework\")\n",
    "    print(\"- Set up model drift detection and alerts\")\n",
    "    \n",
    "    print(\"\\n## 6. API Endpoints\")\n",
    "    print(\"- Period prediction endpoints\")\n",
    "    print(\"- Fertility and ovulation prediction endpoints\")\n",
    "    print(\"- Training initiation endpoints\")\n",
    "    print(\"- Model management endpoints (list, details, metrics)\")\n",
    "    \n",
    "    return ml_components\n",
    "\n",
    "# Execute the analysis\n",
    "ml_requirements = analyze_ml_requirements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4e74587",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T15:50:23.815986Z",
     "iopub.status.busy": "2025-03-25T15:50:23.814802Z",
     "iopub.status.idle": "2025-03-25T15:50:23.837338Z",
     "shell.execute_reply": "2025-03-25T15:50:23.836567Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated triple-quoted string literal (detected at line 844) (1193827185.py, line 813)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 813\u001b[0;36m\u001b[0m\n\u001b[0;31m    prediction_service_content = \"\"\"from typing import Dict, Any, List\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated triple-quoted string literal (detected at line 844)\n"
     ]
    }
   ],
   "source": [
    "I'll create the basic FastAPI application structure for the period tracker ML engine. I'll organize it with separate directories for models, preprocessing, training, API endpoints, and utilities, following best practices for a production-ready system.\n",
    "\n",
    "```python\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Create the project directory structure for the FastAPI ML engine\n",
    "def create_project_structure():\n",
    "    \"\"\"Create the project directory structure for the FastAPI ML engine\"\"\"\n",
    "    # Define root directory for the ML engine\n",
    "    ml_engine_dir = Path(\"period_tracker_ml\")\n",
    "    \n",
    "    # Define subdirectories\n",
    "    directories = [\n",
    "        \"app\",\n",
    "        \"app/api\",\n",
    "        \"app/api/v1\",\n",
    "        \"app/api/v1/endpoints\",\n",
    "        \"app/core\",\n",
    "        \"app/db\",\n",
    "        \"app/models\",\n",
    "        \"app/models/arima\",\n",
    "        \"app/models/random_forest\",\n",
    "        \"app/models/gradient_boosting\",\n",
    "        \"app/models/ensemble\",\n",
    "        \"app/preprocessing\",\n",
    "        \"app/schemas\",\n",
    "        \"app/training\",\n",
    "        \"app/utils\",\n",
    "        \"tests\",\n",
    "        \"data\"\n",
    "    ]\n",
    "    \n",
    "    # Create directories if they don't exist\n",
    "    for directory in directories:\n",
    "        dir_path = ml_engine_dir / directory\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        # Create an empty __init__.py file in each directory for proper Python packaging\n",
    "        if directory.startswith(\"app\"):\n",
    "            init_file = dir_path / \"__init__.py\"\n",
    "            if not init_file.exists():\n",
    "                with open(init_file, \"w\") as f:\n",
    "                    pass\n",
    "    \n",
    "    print(f\"Created directory structure for the ML engine at: {ml_engine_dir.absolute()}\")\n",
    "    return ml_engine_dir\n",
    "\n",
    "# Create the project structure\n",
    "ml_engine_dir = create_project_structure()\n",
    "\n",
    "# Create main.py at the root of the application\n",
    "def create_main_file(ml_engine_dir):\n",
    "    \"\"\"Create the main.py file for the FastAPI application\"\"\"\n",
    "    main_content = \"\"\"from fastapi import FastAPI, Depends\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "import mlflow\n",
    "import uvicorn\n",
    "\n",
    "from app.api.v1.api import api_router\n",
    "from app.core.config import settings\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI(\n",
    "    title=settings.PROJECT_NAME,\n",
    "    description=\"Period Tracker ML Engine API\",\n",
    "    version=\"1.0.0\",\n",
    "    docs_url=\"/docs\",\n",
    "    redoc_url=\"/redoc\",\n",
    ")\n",
    "\n",
    "# Set up CORS middleware\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=settings.CORS_ORIGINS,\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Include API router\n",
    "app.include_router(api_router, prefix=settings.API_V1_STR)\n",
    "\n",
    "# Root endpoint\n",
    "@app.get(\"/\")\n",
    "def root():\n",
    "    return {\n",
    "        \"message\": \"Welcome to Period Tracker ML Engine API\",\n",
    "        \"docs\": \"/docs\",\n",
    "        \"status\": \"operational\"\n",
    "    }\n",
    "\n",
    "# Health check endpoint\n",
    "@app.get(\"/health\")\n",
    "def health():\n",
    "    return {\"status\": \"healthy\"}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(\"main:app\", host=\"0.0.0.0\", port=8000, reload=True)\n",
    "\"\"\"\n",
    "    \n",
    "    with open(ml_engine_dir / \"main.py\", \"w\") as f:\n",
    "        f.write(main_content)\n",
    "    \n",
    "    print(f\"Created main.py at {ml_engine_dir / 'main.py'}\")\n",
    "\n",
    "# Create configuration files\n",
    "def create_config_files(ml_engine_dir):\n",
    "    \"\"\"Create configuration files for the application\"\"\"\n",
    "    # Create settings.py in the core directory\n",
    "    settings_content = \"\"\"from typing import List, Union, Dict, Any, Optional\n",
    "from pydantic import BaseSettings, validator, PostgresDsn\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "class Settings(BaseSettings):\n",
    "    # API settings\n",
    "    API_V1_STR: str = \"/api/v1\"\n",
    "    PROJECT_NAME: str = \"Period Tracker ML Engine\"\n",
    "    \n",
    "    # CORS settings\n",
    "    CORS_ORIGINS: List[str] = [\"*\"]  # In production, replace with actual origins\n",
    "    \n",
    "    # Security settings\n",
    "    SECRET_KEY: str = \"dev_secret_key_change_in_production\"  # Change in production\n",
    "    ACCESS_TOKEN_EXPIRE_MINUTES: int = 60 * 24 * 7  # 7 days\n",
    "    \n",
    "    # Database settings\n",
    "    POSTGRES_SERVER: str = \"localhost\"\n",
    "    POSTGRES_USER: str = \"postgres\"\n",
    "    POSTGRES_PASSWORD: str = \"postgres\"\n",
    "    POSTGRES_DB: str = \"periodtracker\"\n",
    "    POSTGRES_PORT: str = \"5432\"\n",
    "    SQLALCHEMY_DATABASE_URI: Optional[PostgresDsn] = None\n",
    "    \n",
    "    @validator(\"SQLALCHEMY_DATABASE_URI\", pre=True)\n",
    "    def assemble_db_connection(cls, v: Optional[str], values: Dict[str, Any]) -> Any:\n",
    "        if isinstance(v, str):\n",
    "            return v\n",
    "        return PostgresDsn.build(\n",
    "            scheme=\"postgresql\",\n",
    "            user=values.get(\"POSTGRES_USER\"),\n",
    "            password=values.get(\"POSTGRES_PASSWORD\"),\n",
    "            host=values.get(\"POSTGRES_SERVER\"),\n",
    "            port=values.get(\"POSTGRES_PORT\"),\n",
    "            path=f\"/{values.get('POSTGRES_DB') or ''}\",\n",
    "        )\n",
    "    \n",
    "    # MLflow settings\n",
    "    MLFLOW_TRACKING_URI: str = \"sqlite:///mlruns.db\"\n",
    "    MLFLOW_EXPERIMENT_NAME: str = \"period-tracker-models\"\n",
    "    \n",
    "    # Model settings\n",
    "    MODEL_PATH: Path = Path(\"models\")\n",
    "    DEFAULT_MODEL_VERSION: str = \"v1.0.0\"\n",
    "    \n",
    "    # Training settings\n",
    "    GLOBAL_MODEL_TRAINING_SCHEDULE: str = \"0 0 * * 0\"  # Weekly at midnight on Sunday\n",
    "    USER_MODEL_TRAINING_SCHEDULE: str = \"0 0 1 * *\"    # Monthly at midnight on the 1st\n",
    "    MIN_CYCLES_FOR_USER_MODEL: int = 3  # Minimum cycles needed for user-specific model\n",
    "    \n",
    "    # Performance thresholds\n",
    "    MIN_ACCURACY_THRESHOLD: float = 0.80  # Min accuracy to deploy a new model\n",
    "    ACCURACY_IMPROVEMENT_THRESHOLD: float = 0.02  # Required improvement for new model\n",
    "    \n",
    "    class Config:\n",
    "        case_sensitive = True\n",
    "        env_file = \".env\"\n",
    "\n",
    "settings = Settings()\n",
    "\"\"\"\n",
    "    \n",
    "    with open(ml_engine_dir / \"app\" / \"core\" / \"config.py\", \"w\") as f:\n",
    "        f.write(settings_content)\n",
    "    \n",
    "    # Create dependencies.py in the core directory\n",
    "    dependencies_content = \"\"\"from fastapi import Depends, HTTPException, status\n",
    "from fastapi.security import APIKeyHeader\n",
    "from sqlalchemy.orm import Session\n",
    "import jwt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from app.core.config import settings\n",
    "from app.db.session import SessionLocal\n",
    "\n",
    "# Database dependency\n",
    "def get_db():\n",
    "    db = SessionLocal()\n",
    "    try:\n",
    "        yield db\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "# API Key header for authentication\n",
    "api_key_header = APIKeyHeader(name=\"X-API-Key\", auto_error=False)\n",
    "\n",
    "# Authentication dependency\n",
    "def get_api_key(api_key: str = Depends(api_key_header)):\n",
    "    if not api_key:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_401_UNAUTHORIZED,\n",
    "            detail=\"API key is missing\",\n",
    "        )\n",
    "    \n",
    "    # In production, validate against database of valid API keys\n",
    "    if api_key != \"test_api_key\":  # Replace with actual validation\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_403_FORBIDDEN,\n",
    "            detail=\"Invalid API key\",\n",
    "        )\n",
    "    \n",
    "    return api_key\n",
    "\"\"\"\n",
    "    \n",
    "    with open(ml_engine_dir / \"app\" / \"core\" / \"dependencies.py\", \"w\") as f:\n",
    "        f.write(dependencies_content)\n",
    "    \n",
    "    print(f\"Created configuration files in {ml_engine_dir / 'app' / 'core'}\")\n",
    "\n",
    "# Create API router files\n",
    "def create_api_files(ml_engine_dir):\n",
    "    \"\"\"Create API router files\"\"\"\n",
    "    # Create api.py in the api/v1 directory\n",
    "    api_content = \"\"\"from fastapi import APIRouter\n",
    "\n",
    "from app.api.v1.endpoints import predictions, training, models\n",
    "\n",
    "api_router = APIRouter()\n",
    "\n",
    "api_router.include_router(predictions.router, prefix=\"/predictions\", tags=[\"predictions\"])\n",
    "api_router.include_router(training.router, prefix=\"/training\", tags=[\"training\"])\n",
    "api_router.include_router(models.router, prefix=\"/models\", tags=[\"models\"])\n",
    "\"\"\"\n",
    "    \n",
    "    with open(ml_engine_dir / \"app\" / \"api\" / \"v1\" / \"api.py\", \"w\") as f:\n",
    "        f.write(api_content)\n",
    "    \n",
    "    # Create predictions.py endpoint\n",
    "    predictions_content = \"\"\"from fastapi import APIRouter, Depends, HTTPException, status\n",
    "from typing import Dict, Any, List\n",
    "from sqlalchemy.orm import Session\n",
    "import pandas as pd\n",
    "from datetime import date, datetime\n",
    "\n",
    "from app.core.dependencies import get_db, get_api_key\n",
    "from app.models.ensemble.prediction_service import PredictionService\n",
    "from app.schemas.prediction import PeriodPredictionRequest, PeriodPredictionResponse\n",
    "from app.schemas.prediction import FertilityPredictionRequest, FertilityPredictionResponse\n",
    "from app.schemas.prediction import OvulationPredictionRequest, OvulationPredictionResponse\n",
    "\n",
    "router = APIRouter()\n",
    "prediction_service = PredictionService()\n",
    "\n",
    "@router.post(\"/period\", response_model=PeriodPredictionResponse)\n",
    "async def predict_period(\n",
    "    request: PeriodPredictionRequest,\n",
    "    api_key: str = Depends(get_api_key),\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \\\"\\\"\\\"Generate period predictions based on user's cycle history\\\"\\\"\\\"\n",
    "    try:\n",
    "        # Here we would fetch additional data from database if needed\n",
    "        result = prediction_service.predict_period(request.dict())\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        # Log the error\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "            detail=f\"Prediction error: {str(e)}\"\n",
    "        )\n",
    "\n",
    "@router.post(\"/fertility\", response_model=FertilityPredictionResponse)\n",
    "async def predict_fertility(\n",
    "    request: FertilityPredictionRequest,\n",
    "    api_key: str = Depends(get_api_key),\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \\\"\\\"\\\"Generate fertility window predictions\\\"\\\"\\\"\n",
    "    try:\n",
    "        result = prediction_service.predict_fertility(request.dict())\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        # Log the error\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "            detail=f\"Prediction error: {str(e)}\"\n",
    "        )\n",
    "\n",
    "@router.post(\"/ovulation\", response_model=OvulationPredictionResponse)\n",
    "async def predict_ovulation(\n",
    "    request: OvulationPredictionRequest,\n",
    "    api_key: str = Depends(get_api_key),\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \\\"\\\"\\\"Generate ovulation day predictions\\\"\\\"\\\"\n",
    "    try:\n",
    "        result = prediction_service.predict_ovulation(request.dict())\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        # Log the error\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "            detail=f\"Prediction error: {str(e)}\"\n",
    "        )\n",
    "\"\"\"\n",
    "    \n",
    "    with open(ml_engine_dir / \"app\" / \"api\" / \"v1\" / \"endpoints\" / \"predictions.py\", \"w\") as f:\n",
    "        f.write(predictions_content)\n",
    "    \n",
    "    # Create training.py endpoint\n",
    "    training_content = \"\"\"from fastapi import APIRouter, Depends, HTTPException, status, BackgroundTasks\n",
    "from typing import Dict, Any, List\n",
    "from sqlalchemy.orm import Session\n",
    "\n",
    "from app.core.dependencies import get_db, get_api_key\n",
    "from app.training.model_trainer import ModelTrainer\n",
    "from app.schemas.training import TrainingRequest, TrainingResponse\n",
    "\n",
    "router = APIRouter()\n",
    "model_trainer = ModelTrainer()\n",
    "\n",
    "@router.post(\"/global\", response_model=TrainingResponse)\n",
    "async def train_global_model(\n",
    "    request: TrainingRequest,\n",
    "    background_tasks: BackgroundTasks,\n",
    "    api_key: str = Depends(get_api_key),\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \\\"\\\"\\\"Trigger training of a global model\\\"\\\"\\\"\n",
    "    try:\n",
    "        # Start training in background task\n",
    "        background_tasks.add_task(\n",
    "            model_trainer.train_global_model,\n",
    "            model_type=request.model_type\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"training_started\",\n",
    "            \"model_type\": request.model_type,\n",
    "            \"message\": f\"Global {request.model_type} model training initiated\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        # Log the error\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "            detail=f\"Training error: {str(e)}\"\n",
    "        )\n",
    "\n",
    "@router.post(\"/user/{user_id}\", response_model=TrainingResponse)\n",
    "async def train_user_model(\n",
    "    user_id: int,\n",
    "    request: TrainingRequest,\n",
    "    background_tasks: BackgroundTasks,\n",
    "    api_key: str = Depends(get_api_key),\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \\\"\\\"\\\"Trigger training of a user-specific model\\\"\\\"\\\"\n",
    "    try:\n",
    "        # Start training in background task\n",
    "        background_tasks.add_task(\n",
    "            model_trainer.train_user_model,\n",
    "            user_id=user_id,\n",
    "            model_type=request.model_type\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"training_started\",\n",
    "            \"model_type\": request.model_type,\n",
    "            \"user_id\": user_id,\n",
    "            \"message\": f\"User-specific {request.model_type} model training initiated for user {user_id}\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        # Log the error\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "            detail=f\"Training error: {str(e)}\"\n",
    "        )\n",
    "\"\"\"\n",
    "    \n",
    "    with open(ml_engine_dir / \"app\" / \"api\" / \"v1\" / \"endpoints\" / \"training.py\", \"w\") as f:\n",
    "        f.write(training_content)\n",
    "    \n",
    "    # Create models.py endpoint\n",
    "    models_content = \"\"\"from fastapi import APIRouter, Depends, HTTPException, status\n",
    "from typing import Dict, Any, List\n",
    "from sqlalchemy.orm import Session\n",
    "\n",
    "from app.core.dependencies import get_db, get_api_key\n",
    "from app.schemas.ml_model import ModelInfo, ModelMetrics, ModelList\n",
    "import mlflow\n",
    "\n",
    "router = APIRouter()\n",
    "\n",
    "@router.get(\"\", response_model=ModelList)\n",
    "async def list_models(\n",
    "    api_key: str = Depends(get_api_key),\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \\\"\\\"\\\"List all ML model versions\\\"\\\"\\\"\n",
    "    try:\n",
    "        # Here we would fetch model info from MLflow\n",
    "        # This is a placeholder implementation\n",
    "        models = [\n",
    "            {\n",
    "                \"id\": 1,\n",
    "                \"name\": \"arima_global\",\n",
    "                \"version\": \"v1.0.0\",\n",
    "                \"is_active\": True,\n",
    "                \"is_global\": True,\n",
    "                \"accuracy\": 0.87,\n",
    "                \"trained_at\": \"2023-01-01T00:00:00Z\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": 2,\n",
    "                \"name\": \"rf_global\",\n",
    "                \"version\": \"v1.0.0\",\n",
    "                \"is_active\": True,\n",
    "                \"is_global\": True,\n",
    "                \"accuracy\": 0.85,\n",
    "                \"trained_at\": \"2023-01-01T00:00:00Z\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        return {\"models\": models}\n",
    "    except Exception as e:\n",
    "        # Log the error\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "            detail=f\"Error listing models: {str(e)}\"\n",
    "        )\n",
    "\n",
    "@router.get(\"/{model_id}\", response_model=ModelInfo)\n",
    "async def get_model_details(\n",
    "    model_id: int,\n",
    "    api_key: str = Depends(get_api_key),\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \\\"\\\"\\\"Get details about a specific model\\\"\\\"\\\"\n",
    "    try:\n",
    "        # Here we would fetch model info from MLflow\n",
    "        # This is a placeholder implementation\n",
    "        if model_id == 1:\n",
    "            return {\n",
    "                \"id\": 1,\n",
    "                \"name\": \"arima_global\",\n",
    "                \"type\": \"ARIMA\",\n",
    "                \"version\": \"v1.0.0\",\n",
    "                \"is_active\": True,\n",
    "                \"is_global\": True,\n",
    "                \"user_id\": None,\n",
    "                \"accuracy\": 0.87,\n",
    "                \"file_path\": \"models/arima/v1.0.0\",\n",
    "                \"trained_at\": \"2023-01-01T00:00:00Z\",\n",
    "                \"created_at\": \"2023-01-01T00:00:00Z\"\n",
    "            }\n",
    "        else:\n",
    "            raise HTTPException(\n",
    "                status_code=status.HTTP_404_NOT_FOUND,\n",
    "                detail=f\"Model with id {model_id} not found\"\n",
    "            )\n",
    "    except HTTPException as e:\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        # Log the error\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "            detail=f\"Error getting model details: {str(e)}\"\n",
    "        )\n",
    "\n",
    "@router.get(\"/{model_id}/metrics\", response_model=ModelMetrics)\n",
    "async def get_model_metrics(\n",
    "    model_id: int,\n",
    "    api_key: str = Depends(get_api_key),\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \\\"\\\"\\\"Get performance metrics for a specific model\\\"\\\"\\\"\n",
    "    try:\n",
    "        # Here we would fetch model metrics from MLflow\n",
    "        # This is a placeholder implementation\n",
    "        if model_id == 1:\n",
    "            return {\n",
    "                \"id\": 1,\n",
    "                \"model_id\": 1,\n",
    "                \"mae\": 1.2,\n",
    "                \"rmse\": 1.8,\n",
    "                \"accuracy\": 0.87,\n",
    "                \"sample_size\": 1000,\n",
    "                \"created_at\": \"2023-01-01T00:00:00Z\"\n",
    "            }\n",
    "        else:\n",
    "            raise HTTPException(\n",
    "                status_code=status.HTTP_404_NOT_FOUND,\n",
    "                detail=f\"Model with id {model_id} not found\"\n",
    "            )\n",
    "    except HTTPException as e:\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        # Log the error\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "            detail=f\"Error getting model metrics: {str(e)}\"\n",
    "        )\n",
    "\n",
    "@router.put(\"/{model_id}/activate\", response_model=ModelInfo)\n",
    "async def activate_model(\n",
    "    model_id: int,\n",
    "    api_key: str = Depends(get_api_key),\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \\\"\\\"\\\"Activate a specific model version\\\"\\\"\\\"\n",
    "    try:\n",
    "        # Here we would activate the model in MLflow\n",
    "        # This is a placeholder implementation\n",
    "        if model_id == 1 or model_id == 2:\n",
    "            return {\n",
    "                \"id\": model_id,\n",
    "                \"name\": \"arima_global\" if model_id == 1 else \"rf_global\",\n",
    "                \"type\": \"ARIMA\" if model_id == 1 else \"RandomForest\",\n",
    "                \"version\": \"v1.0.0\",\n",
    "                \"is_active\": True,\n",
    "                \"is_global\": True,\n",
    "                \"user_id\": None,\n",
    "                \"accuracy\": 0.87 if model_id == 1 else 0.85,\n",
    "                \"file_path\": f\"models/{'arima' if model_id == 1 else 'rf'}/v1.0.0\",\n",
    "                \"trained_at\": \"2023-01-01T00:00:00Z\",\n",
    "                \"created_at\": \"2023-01-01T00:00:00Z\"\n",
    "            }\n",
    "        else:\n",
    "            raise HTTPException(\n",
    "                status_code=status.HTTP_404_NOT_FOUND,\n",
    "                detail=f\"Model with id {model_id} not found\"\n",
    "            )\n",
    "    except HTTPException as e:\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        # Log the error\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "            detail=f\"Error activating model: {str(e)}\"\n",
    "        )\n",
    "\"\"\"\n",
    "    \n",
    "    with open(ml_engine_dir / \"app\" / \"api\" / \"v1\" / \"endpoints\" / \"models.py\", \"w\") as f:\n",
    "        f.write(models_content)\n",
    "    \n",
    "    print(f\"Created API router files in {ml_engine_dir / 'app' / 'api' / 'v1'}\")\n",
    "\n",
    "# Create schema definitions\n",
    "def create_schema_files(ml_engine_dir):\n",
    "    \"\"\"Create Pydantic schema files for data validation\"\"\"\n",
    "    # Create prediction.py schema\n",
    "    prediction_schema_content = \"\"\"from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Dict, Any\n",
    "from datetime import date, datetime\n",
    "\n",
    "# Period Prediction\n",
    "class PeriodPredictionRequest(BaseModel):\n",
    "    user_id: int\n",
    "    historical_cycles: List[Dict[str, Any]]\n",
    "    current_symptoms: Optional[List[Dict[str, Any]]] = []\n",
    "    last_period_start: date\n",
    "    average_cycle_length: Optional[int] = None\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"user_id\": 1,\n",
    "                \"historical_cycles\": [\n",
    "                    {\"start_date\": \"2023-01-01\", \"end_date\": \"2023-01-05\", \"duration\": 5},\n",
    "                    {\"start_date\": \"2023-01-28\", \"end_date\": \"2023-02-02\", \"duration\": 6}\n",
    "                ],\n",
    "                \"current_symptoms\": [\n",
    "                    {\"symptom_id\": 1, \"severity\": 3, \"date\": \"2023-02-25\"}\n",
    "                ],\n",
    "                \"last_period_start\": \"2023-01-28\",\n",
    "                \"average_cycle_length\": 28\n",
    "            }\n",
    "        }\n",
    "\n",
    "class PeriodPredictionResponse(BaseModel):\n",
    "    next_period_date: date\n",
    "    confidence_score: float\n",
    "    predicted_duration: int\n",
    "    window_start: date\n",
    "    window_end: date\n",
    "    model_version: str\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"next_period_date\": \"2023-02-25\",\n",
    "                \"confidence_score\": 0.88,\n",
    "                \"predicted_duration\": 5,\n",
    "                \"window_start\": \"2023-02-24\",\n",
    "                \"window_end\": \"2023-02-26\",\n",
    "                \"model_version\": \"v1.0.0\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Fertility Prediction\n",
    "class FertilityPredictionRequest(BaseModel):\n",
    "    user_id: int\n",
    "    historical_cycles: List[Dict[str, Any]]\n",
    "    basal_temperatures: Optional[List[Dict[str, Any]]] = []\n",
    "    cervical_fluid: Optional[List[Dict[str, Any]]] = []\n",
    "    ovulation_tests: Optional[List[Dict[str, Any]]] = []\n",
    "    last_period_start: date\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"user_id\": 1,\n",
    "                \"historical_cycles\": [\n",
    "                    {\"start_date\": \"2023-01-01\", \"end_date\": \"2023-01-05\", \"duration\": 5},\n",
    "                    {\"start_date\": \"2023-01-28\", \"end_date\": \"2023-02-02\", \"duration\": 6}\n",
    "                ],\n",
    "                \"basal_temperatures\": [\n",
    "                    {\"date\": \"2023-02-10\", \"value\": 36.5},\n",
    "                    {\"date\": \"2023-02-11\", \"value\": 36.7}\n",
    "                ],\n",
    "                \"last_period_start\": \"2023-01-28\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "class FertilityPredictionResponse(BaseModel):\n",
    "    fertility_window_start: date\n",
    "    fertility_window_end: date\n",
    "    peak_fertility_date: date\n",
    "    confidence_score: float\n",
    "    model_version: str\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"fertility_window_start\": \"2023-02-08\",\n",
    "                \"fertility_window_end\": \"2023-02-13\",\n",
    "                \"peak_fertility_date\": \"2023-02-11\",\n",
    "                \"confidence_score\": 0.82,\n",
    "                \"model_version\": \"v1.0.0\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Ovulation Prediction\n",
    "class OvulationPredictionRequest(BaseModel):\n",
    "    user_id: int\n",
    "    historical_cycles: List[Dict[str, Any]]\n",
    "    basal_temperatures: Optional[List[Dict[str, Any]]] = []\n",
    "    cervical_fluid: Optional[List[Dict[str, Any]]] = []\n",
    "    ovulation_tests: Optional[List[Dict[str, Any]]] = []\n",
    "    last_period_start: date\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"user_id\": 1,\n",
    "                \"historical_cycles\": [\n",
    "                    {\"start_date\": \"2023-01-01\", \"end_date\": \"2023-01-05\", \"duration\": 5},\n",
    "                    {\"start_date\": \"2023-01-28\", \"end_date\": \"2023-02-02\", \"duration\": 6}\n",
    "                ],\n",
    "                \"basal_temperatures\": [\n",
    "                    {\"date\": \"2023-02-10\", \"value\": 36.5},\n",
    "                    {\"date\": \"2023-02-11\", \"value\": 36.7}\n",
    "                ],\n",
    "                \"last_period_start\": \"2023-01-28\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "class OvulationPredictionResponse(BaseModel):\n",
    "    ovulation_date: date\n",
    "    confidence_score: float\n",
    "    model_version: str\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"ovulation_date\": \"2023-02-11\",\n",
    "                \"confidence_score\": 0.85,\n",
    "                \"model_version\": \"v1.0.0\"\n",
    "            }\n",
    "        }\n",
    "\"\"\"\n",
    "    \n",
    "    with open(ml_engine_dir / \"app\" / \"schemas\" / \"prediction.py\", \"w\") as f:\n",
    "        f.write(prediction_schema_content)\n",
    "    \n",
    "    # Create training.py schema\n",
    "    training_schema_content = \"\"\"from pydantic import BaseModel, Field\n",
    "from typing import Optional, List\n",
    "from datetime import datetime\n",
    "\n",
    "class TrainingRequest(BaseModel):\n",
    "    model_type: str = Field(..., description=\"Type of model to train: 'period', 'fertility', 'ovulation', 'all'\")\n",
    "    force: Optional[bool] = False\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"model_type\": \"period\",\n",
    "                \"force\": False\n",
    "            }\n",
    "        }\n",
    "\n",
    "class TrainingResponse(BaseModel):\n",
    "    status: str\n",
    "    model_type: str\n",
    "    user_id: Optional[int] = None\n",
    "    message: str\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"status\": \"training_started\",\n",
    "                \"model_type\": \"period\",\n",
    "                \"message\": \"Global period model training initiated\"\n",
    "            }\n",
    "        }\n",
    "\"\"\"\n",
    "    \n",
    "    with open(ml_engine_dir / \"app\" / \"schemas\" / \"training.py\", \"w\") as f:\n",
    "        f.write(training_schema_content)\n",
    "    \n",
    "    # Create ml_model.py schema\n",
    "    ml_model_schema_content = \"\"\"from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "class ModelInfo(BaseModel):\n",
    "    id: int\n",
    "    name: str\n",
    "    type: str\n",
    "    version: str\n",
    "    is_active: bool\n",
    "    is_global: bool\n",
    "    user_id: Optional[int] = None\n",
    "    accuracy: float\n",
    "    file_path: str\n",
    "    trained_at: datetime\n",
    "    created_at: datetime\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"id\": 1,\n",
    "                \"name\": \"arima_global\",\n",
    "                \"type\": \"ARIMA\",\n",
    "                \"version\": \"v1.0.0\",\n",
    "                \"is_active\": True,\n",
    "                \"is_global\": True,\n",
    "                \"user_id\": None,\n",
    "                \"accuracy\": 0.87,\n",
    "                \"file_path\": \"models/arima/v1.0.0\",\n",
    "                \"trained_at\": \"2023-01-01T00:00:00Z\",\n",
    "                \"created_at\": \"2023-01-01T00:00:00Z\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "class ModelMetrics(BaseModel):\n",
    "    id: int\n",
    "    model_id: int\n",
    "    mae: float\n",
    "    rmse: float\n",
    "    accuracy: float\n",
    "    sample_size: int\n",
    "    created_at: datetime\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"id\": 1,\n",
    "                \"model_id\": 1,\n",
    "                \"mae\": 1.2,\n",
    "                \"rmse\": 1.8,\n",
    "                \"accuracy\": 0.87,\n",
    "                \"sample_size\": 1000,\n",
    "                \"created_at\": \"2023-01-01T00:00:00Z\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "class ModelList(BaseModel):\n",
    "    models: List[ModelInfo]\n",
    "\"\"\"\n",
    "    \n",
    "    with open(ml_engine_dir / \"app\" / \"schemas\" / \"ml_model.py\", \"w\") as f:\n",
    "        f.write(ml_model_schema_content)\n",
    "    \n",
    "    print(f\"Created schema files in {ml_engine_dir / 'app' / 'schemas'}\")\n",
    "\n",
    "# Create database session management\n",
    "def create_db_files(ml_engine_dir):\n",
    "    \"\"\"Create database session management files\"\"\"\n",
    "    # Create session.py\n",
    "    session_content = \"\"\"from sqlalchemy import create_engine\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "from app.core.config import settings\n",
    "\n",
    "engine = create_engine(str(settings.SQLALCHEMY_DATABASE_URI))\n",
    "SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
    "\n",
    "Base = declarative_base()\n",
    "\"\"\"\n",
    "    \n",
    "    with open(ml_engine_dir / \"app\" / \"db\" / \"session.py\", \"w\") as f:\n",
    "        f.write(session_content)\n",
    "    \n",
    "    print(f\"Created database session files in {ml_engine_dir / 'app' / 'db'}\")\n",
    "\n",
    "# Create model component files with placeholder implementations\n",
    "def create_model_component_files(ml_engine_dir):\n",
    "    \"\"\"Create placeholder files for model components\"\"\"\n",
    "    # Create prediction_service.py in ensemble directory\n",
    "    prediction_service_content = \"\"\"from typing import Dict, Any, List\n",
    "from datetime import date, datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from app.models.arima.arima_model import ARIMAModel\n",
    "from app.models.random_forest.rf_model import RandomForestModel\n",
    "from app.models.gradient_boosting.gb_model import GradientBoostingModel\n",
    "from app.models.ensemble.ensemble_integrator import EnsembleIntegrator\n",
    "\n",
    "class PredictionService:\n",
    "    def __init__(self):\n",
    "        self.arima_model = ARIMAModel()\n",
    "        self.rf_model = RandomForestModel()\n",
    "        self.gb_model = GradientBoostingModel()\n",
    "        self.ensemble_integrator = EnsembleIntegrator()\n",
    "    \n",
    "    def predict_period(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \\\"\\\"\\\"\n",
    "        Generate period predictions using ensemble model\n",
    "        \n",
    "        Args:\n",
    "            data: Dict containing user's cycle history and current status\n",
    "            \n",
    "        Returns:\n",
    "            Dict with period predictions and confidence score\n",
    "        \\\"\\\"\\\"\n",
    "        # Convert input data to features\n",
    "        # This is a placeholder implementation\n",
    "        user_id = data.get(\"user_id\")\n",
    "        last_period = data.get(\"last_period_start\")\n",
    "        avg_cycle_length = data.get(\"average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1e74271",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T15:52:16.798199Z",
     "iopub.status.busy": "2025-03-25T15:52:16.797966Z",
     "iopub.status.idle": "2025-03-25T15:52:16.824703Z",
     "shell.execute_reply": "2025-03-25T15:52:16.824058Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 4) (3842778014.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    The error occurred in the `create_model_component_files()` function when creating the `prediction_service.py` file. There was an unterminated triple-quoted string in the `prediction_service_content` variable. The issue happened at line 813 where I started defining the `predict_period` method, but the string was cut off at line 844 where I was writing `avg_cycle_length = data.get(\"average` without completing the string properly.\u001b[0m\n\u001b[0m                                                                                                                                                                                                                                                                                                                                                                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 4)\n"
     ]
    }
   ],
   "source": [
    "[reflection on previous impl]\n",
    "In my previous implementation, I was setting up the project structure for the FastAPI ML engine for the period tracker app, which included creating directories, configuration files, API routers, schemas, and model component placeholders. \n",
    "\n",
    "The error occurred in the `create_model_component_files()` function when creating the `prediction_service.py` file. There was an unterminated triple-quoted string in the `prediction_service_content` variable. The issue happened at line 813 where I started defining the `predict_period` method, but the string was cut off at line 844 where I was writing `avg_cycle_length = data.get(\"average` without completing the string properly.\n",
    "\n",
    "[improved impl]:\n",
    "```python\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from metagpt.tools.libs.terminal import Terminal\n",
    "\n",
    "# Install required packages\n",
    "terminal = Terminal()\n",
    "# await terminal.run('pip install fastapi uvicorn sqlalchemy pydantic mlflow')\n",
    "\n",
    "# Create the project directory structure for the FastAPI ML engine\n",
    "def create_project_structure():\n",
    "    \"\"\"Create the project directory structure for the FastAPI ML engine\"\"\"\n",
    "    # Define root directory for the ML engine\n",
    "    ml_engine_dir = Path(\"period_tracker_ml\")\n",
    "    \n",
    "    # Define subdirectories\n",
    "    directories = [\n",
    "        \"app\",\n",
    "        \"app/api\",\n",
    "        \"app/api/v1\",\n",
    "        \"app/api/v1/endpoints\",\n",
    "        \"app/core\",\n",
    "        \"app/db\",\n",
    "        \"app/models\",\n",
    "        \"app/models/arima\",\n",
    "        \"app/models/random_forest\",\n",
    "        \"app/models/gradient_boosting\",\n",
    "        \"app/models/ensemble\",\n",
    "        \"app/preprocessing\",\n",
    "        \"app/schemas\",\n",
    "        \"app/training\",\n",
    "        \"app/utils\",\n",
    "        \"tests\",\n",
    "        \"data\"\n",
    "    ]\n",
    "    \n",
    "    # Create directories if they don't exist\n",
    "    for directory in directories:\n",
    "        dir_path = ml_engine_dir / directory\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        # Create an empty __init__.py file in each directory for proper Python packaging\n",
    "        if directory.startswith(\"app\"):\n",
    "            init_file = dir_path / \"__init__.py\"\n",
    "            if not init_file.exists():\n",
    "                with open(init_file, \"w\") as f:\n",
    "                    pass\n",
    "    \n",
    "    print(f\"Created directory structure for the ML engine at: {ml_engine_dir.absolute()}\")\n",
    "    return ml_engine_dir\n",
    "\n",
    "# Create the project structure\n",
    "ml_engine_dir = create_project_structure()\n",
    "\n",
    "# Create main.py at the root of the application\n",
    "def create_main_file(ml_engine_dir):\n",
    "    \"\"\"Create the main.py file for the FastAPI application\"\"\"\n",
    "    main_content = \"\"\"from fastapi import FastAPI, Depends\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "import mlflow\n",
    "import uvicorn\n",
    "\n",
    "from app.api.v1.api import api_router\n",
    "from app.core.config import settings\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI(\n",
    "    title=settings.PROJECT_NAME,\n",
    "    description=\"Period Tracker ML Engine API\",\n",
    "    version=\"1.0.0\",\n",
    "    docs_url=\"/docs\",\n",
    "    redoc_url=\"/redoc\",\n",
    ")\n",
    "\n",
    "# Set up CORS middleware\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=settings.CORS_ORIGINS,\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Include API router\n",
    "app.include_router(api_router, prefix=settings.API_V1_STR)\n",
    "\n",
    "# Root endpoint\n",
    "@app.get(\"/\")\n",
    "def root():\n",
    "    return {\n",
    "        \"message\": \"Welcome to Period Tracker ML Engine API\",\n",
    "        \"docs\": \"/docs\",\n",
    "        \"status\": \"operational\"\n",
    "    }\n",
    "\n",
    "# Health check endpoint\n",
    "@app.get(\"/health\")\n",
    "def health():\n",
    "    return {\"status\": \"healthy\"}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(\"main:app\", host=\"0.0.0.0\", port=8000, reload=True)\n",
    "\"\"\"\n",
    "    \n",
    "    with open(ml_engine_dir / \"main.py\", \"w\") as f:\n",
    "        f.write(main_content)\n",
    "    \n",
    "    print(f\"Created main.py at {ml_engine_dir / 'main.py'}\")\n",
    "\n",
    "# Create configuration files\n",
    "def create_config_files(ml_engine_dir):\n",
    "    \"\"\"Create configuration files for the application\"\"\"\n",
    "    # Create settings.py in the core directory\n",
    "    settings_content = \"\"\"from typing import List, Union, Dict, Any, Optional\n",
    "from pydantic import BaseSettings, validator, PostgresDsn\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "class Settings(BaseSettings):\n",
    "    # API settings\n",
    "    API_V1_STR: str = \"/api/v1\"\n",
    "    PROJECT_NAME: str = \"Period Tracker ML Engine\"\n",
    "    \n",
    "    # CORS settings\n",
    "    CORS_ORIGINS: List[str] = [\"*\"]  # In production, replace with actual origins\n",
    "    \n",
    "    # Security settings\n",
    "    SECRET_KEY: str = \"dev_secret_key_change_in_production\"  # Change in production\n",
    "    ACCESS_TOKEN_EXPIRE_MINUTES: int = 60 * 24 * 7  # 7 days\n",
    "    \n",
    "    # Database settings\n",
    "    POSTGRES_SERVER: str = \"localhost\"\n",
    "    POSTGRES_USER: str = \"postgres\"\n",
    "    POSTGRES_PASSWORD: str = \"postgres\"\n",
    "    POSTGRES_DB: str = \"periodtracker\"\n",
    "    POSTGRES_PORT: str = \"5432\"\n",
    "    SQLALCHEMY_DATABASE_URI: Optional[PostgresDsn] = None\n",
    "    \n",
    "    @validator(\"SQLALCHEMY_DATABASE_URI\", pre=True)\n",
    "    def assemble_db_connection(cls, v: Optional[str], values: Dict[str, Any]) -> Any:\n",
    "        if isinstance(v, str):\n",
    "            return v\n",
    "        return PostgresDsn.build(\n",
    "            scheme=\"postgresql\",\n",
    "            user=values.get(\"POSTGRES_USER\"),\n",
    "            password=values.get(\"POSTGRES_PASSWORD\"),\n",
    "            host=values.get(\"POSTGRES_SERVER\"),\n",
    "            port=values.get(\"POSTGRES_PORT\"),\n",
    "            path=f\"/{values.get('POSTGRES_DB') or ''}\",\n",
    "        )\n",
    "    \n",
    "    # MLflow settings\n",
    "    MLFLOW_TRACKING_URI: str = \"sqlite:///mlruns.db\"\n",
    "    MLFLOW_EXPERIMENT_NAME: str = \"period-tracker-models\"\n",
    "    \n",
    "    # Model settings\n",
    "    MODEL_PATH: Path = Path(\"models\")\n",
    "    DEFAULT_MODEL_VERSION: str = \"v1.0.0\"\n",
    "    \n",
    "    # Training settings\n",
    "    GLOBAL_MODEL_TRAINING_SCHEDULE: str = \"0 0 * * 0\"  # Weekly at midnight on Sunday\n",
    "    USER_MODEL_TRAINING_SCHEDULE: str = \"0 0 1 * *\"    # Monthly at midnight on the 1st\n",
    "    MIN_CYCLES_FOR_USER_MODEL: int = 3  # Minimum cycles needed for user-specific model\n",
    "    \n",
    "    # Performance thresholds\n",
    "    MIN_ACCURACY_THRESHOLD: float = 0.80  # Min accuracy to deploy a new model\n",
    "    ACCURACY_IMPROVEMENT_THRESHOLD: float = 0.02  # Required improvement for new model\n",
    "    \n",
    "    class Config:\n",
    "        case_sensitive = True\n",
    "        env_file = \".env\"\n",
    "\n",
    "settings = Settings()\n",
    "\"\"\"\n",
    "    \n",
    "    with open(ml_engine_dir / \"app\" / \"core\" / \"config.py\", \"w\") as f:\n",
    "        f.write(settings_content)\n",
    "    \n",
    "    # Create dependencies.py in the core directory\n",
    "    dependencies_content = \"\"\"from fastapi import Depends, HTTPException, status\n",
    "from fastapi.security import APIKeyHeader\n",
    "from sqlalchemy.orm import Session\n",
    "import jwt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from app.core.config import settings\n",
    "from app.db.session import SessionLocal\n",
    "\n",
    "# Database dependency\n",
    "def get_db():\n",
    "    db = SessionLocal()\n",
    "    try:\n",
    "        yield db\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "# API Key header for authentication\n",
    "api_key_header = APIKeyHeader(name=\"X-API-Key\", auto_error=False)\n",
    "\n",
    "# Authentication dependency\n",
    "def get_api_key(api_key: str = Depends(api_key_header)):\n",
    "    if not api_key:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_401_UNAUTHORIZED,\n",
    "            detail=\"API key is missing\",\n",
    "        )\n",
    "    \n",
    "    # In production, validate against database of valid API keys\n",
    "    if api_key != \"test_api_key\":  # Replace with actual validation\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_403_FORBIDDEN,\n",
    "            detail=\"Invalid API key\",\n",
    "        )\n",
    "    \n",
    "    return api_key\n",
    "\"\"\"\n",
    "    \n",
    "    with open(ml_engine_dir / \"app\" / \"core\" / \"dependencies.py\", \"w\") as f:\n",
    "        f.write(dependencies_content)\n",
    "    \n",
    "    print(f\"Created configuration files in {ml_engine_dir / 'app' / 'core'}\")\n",
    "\n",
    "# Create API router files\n",
    "def create_api_files(ml_engine_dir):\n",
    "    \"\"\"Create API router files\"\"\"\n",
    "    # Create api.py in the api/v1 directory\n",
    "    api_content = \"\"\"from fastapi import APIRouter\n",
    "\n",
    "from app.api.v1.endpoints import predictions, training, models\n",
    "\n",
    "api_router = APIRouter()\n",
    "\n",
    "api_router.include_router(predictions.router, prefix=\"/predictions\", tags=[\"predictions\"])\n",
    "api_router.include_router(training.router, prefix=\"/training\", tags=[\"training\"])\n",
    "api_router.include_router(models.router, prefix=\"/models\", tags=[\"models\"])\n",
    "\"\"\"\n",
    "    \n",
    "    with open(ml_engine_dir / \"app\" / \"api\" / \"v1\" / \"api.py\", \"w\") as f:\n",
    "        f.write(api_content)\n",
    "    \n",
    "    # Create predictions.py endpoint\n",
    "    predictions_content = \"\"\"from fastapi import APIRouter, Depends, HTTPException, status\n",
    "from typing import Dict, Any, List\n",
    "from sqlalchemy.orm import Session\n",
    "import pandas as pd\n",
    "from datetime import date, datetime\n",
    "\n",
    "from app.core.dependencies import get_db, get_api_key\n",
    "from app.models.ensemble.prediction_service import PredictionService\n",
    "from app.schemas.prediction import PeriodPredictionRequest, PeriodPredictionResponse\n",
    "from app.schemas.prediction import FertilityPredictionRequest, FertilityPredictionResponse\n",
    "from app.schemas.prediction import OvulationPredictionRequest, OvulationPredictionResponse\n",
    "\n",
    "router = APIRouter()\n",
    "prediction_service = PredictionService()\n",
    "\n",
    "@router.post(\"/period\", response_model=PeriodPredictionResponse)\n",
    "async def predict_period(\n",
    "    request: PeriodPredictionRequest,\n",
    "    api_key: str = Depends(get_api_key),\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \\\"\\\"\\\"Generate period predictions based on user's cycle history\\\"\\\"\\\"\n",
    "    try:\n",
    "        # Here we would fetch additional data from database if needed\n",
    "        result = prediction_service.predict_period(request.dict())\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        # Log the error\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "            detail=f\"Prediction error: {str(e)}\"\n",
    "        )\n",
    "\n",
    "@router.post(\"/fertility\", response_model=FertilityPredictionResponse)\n",
    "async def predict_fertility(\n",
    "    request: FertilityPredictionRequest,\n",
    "    api_key: str = Depends(get_api_key),\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \\\"\\\"\\\"Generate fertility window predictions\\\"\\\"\\\"\n",
    "    try:\n",
    "        result = prediction_service.predict_fertility(request.dict())\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        # Log the error\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "            detail=f\"Prediction error: {str(e)}\"\n",
    "        )\n",
    "\n",
    "@router.post(\"/ovulation\", response_model=OvulationPredictionResponse)\n",
    "async def predict_ovulation(\n",
    "    request: OvulationPredictionRequest,\n",
    "    api_key: str = Depends(get_api_key),\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \\\"\\\"\\\"Generate ovulation day predictions\\\"\\\"\\\"\n",
    "    try:\n",
    "        result = prediction_service.predict_ovulation(request.dict())\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        # Log the error\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "            detail=f\"Prediction error: {str(e)}\"\n",
    "        )\n",
    "\"\"\"\n",
    "    \n",
    "    with open(ml_engine_dir / \"app\" / \"api\" / \"v1\" / \"endpoints\" / \"predictions.py\", \"w\") as f:\n",
    "        f.write(predictions_content)\n",
    "    \n",
    "    # Create training.py endpoint\n",
    "    training_content = \"\"\"from fastapi import APIRouter, Depends, HTTPException, status, BackgroundTasks\n",
    "from typing import Dict, Any, List\n",
    "from sqlalchemy.orm import Session\n",
    "\n",
    "from app.core.dependencies import get_db, get_api_key\n",
    "from app.training.model_trainer import ModelTrainer\n",
    "from app.schemas.training import TrainingRequest, TrainingResponse\n",
    "\n",
    "router = APIRouter()\n",
    "model_trainer = ModelTrainer()\n",
    "\n",
    "@router.post(\"/global\", response_model=TrainingResponse)\n",
    "async def train_global_model(\n",
    "    request: TrainingRequest,\n",
    "    background_tasks: BackgroundTasks,\n",
    "    api_key: str = Depends(get_api_key),\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \\\"\\\"\\\"Trigger training of a global model\\\"\\\"\\\"\n",
    "    try:\n",
    "        # Start training in background task\n",
    "        background_tasks.add_task(\n",
    "            model_trainer.train_global_model,\n",
    "            model_type=request.model_type\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"training_started\",\n",
    "            \"model_type\": request.model_type,\n",
    "            \"message\": f\"Global {request.model_type} model training initiated\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        # Log the error\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "            detail=f\"Training error: {str(e)}\"\n",
    "        )\n",
    "\n",
    "@router.post(\"/user/{user_id}\", response_model=TrainingResponse)\n",
    "async def train_user_model(\n",
    "    user_id: int,\n",
    "    request: TrainingRequest,\n",
    "    background_tasks: BackgroundTasks,\n",
    "    api_key: str = Depends(get_api_key),\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \\\"\\\"\\\"Trigger training of a user-specific model\\\"\\\"\\\"\n",
    "    try:\n",
    "        # Start training in background task\n",
    "        background_tasks.add_task(\n",
    "            model_trainer.train_user_model,\n",
    "            user_id=user_id,\n",
    "            model_type=request.model_type\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"training_started\",\n",
    "            \"model_type\": request.model_type,\n",
    "            \"user_id\": user_id,\n",
    "            \"message\": f\"User-specific {request.model_type} model training initiated for user {user_id}\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        # Log the error\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "            detail=f\"Training error: {str(e)}\"\n",
    "        )\n",
    "\"\"\"\n",
    "    \n",
    "    with open(ml_engine_dir / \"app\" / \"api\" / \"v1\" / \"endpoints\" / \"training.py\", \"w\") as f:\n",
    "        f.write(training_content)\n",
    "    \n",
    "    # Create models.py endpoint\n",
    "    models_content = \"\"\"from fastapi import APIRouter, Depends, HTTPException, status\n",
    "from typing import Dict, Any, List\n",
    "from sqlalchemy.orm import Session\n",
    "\n",
    "from app.core.dependencies import get_db, get_api_key\n",
    "from app.schemas.ml_model import ModelInfo, ModelMetrics, ModelList\n",
    "import mlflow\n",
    "\n",
    "router = APIRouter()\n",
    "\n",
    "@router.get(\"\", response_model=ModelList)\n",
    "async def list_models(\n",
    "    api_key: str = Depends(get_api_key),\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \\\"\\\"\\\"List all ML model versions\\\"\\\"\\\"\n",
    "    try:\n",
    "        # Here we would fetch model info from MLflow\n",
    "        # This is a placeholder implementation\n",
    "        models = [\n",
    "            {\n",
    "                \"id\": 1,\n",
    "                \"name\": \"arima_global\",\n",
    "                \"type\": \"ARIMA\",\n",
    "                \"version\": \"v1.0.0\",\n",
    "                \"is_active\": True,\n",
    "                \"is_global\": True,\n",
    "                \"user_id\": None,\n",
    "                \"accuracy\": 0.87,\n",
    "                \"file_path\": \"models/arima/v1.0.0\",\n",
    "                \"trained_at\": \"2023-01-01T00:00:00Z\",\n",
    "                \"created_at\": \"2023-01-01T00:00:00Z\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": 2,\n",
    "                \"name\": \"rf_global\",\n",
    "                \"type\": \"RandomForest\",\n",
    "                \"version\": \"v1.0.0\",\n",
    "                \"is_active\": True,\n",
    "                \"is_global\": True,\n",
    "                \"user_id\": None,\n",
    "                \"accuracy\": 0.85,\n",
    "                \"file_path\": \"models/rf/v1.0.0\",\n",
    "                \"trained_at\": \"2023-01-01T00:00:00Z\",\n",
    "                \"created_at\": \"2023-01-01T00:00:00Z\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        return {\"models\": models}\n",
    "    except Exception as e:\n",
    "        # Log the error\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "            detail=f\"Error listing models: {str(e)}\"\n",
    "        )\n",
    "\n",
    "@router.get(\"/{model_id}\", response_model=ModelInfo)\n",
    "async def get_model_details(\n",
    "    model_id: int,\n",
    "    api_key: str = Depends(get_api_key),\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \\\"\\\"\\\"Get details about a specific model\\\"\\\"\\\"\n",
    "    try:\n",
    "        # Here we would fetch model info from MLflow\n",
    "        # This is a placeholder implementation\n",
    "        if model_id == 1:\n",
    "            return {\n",
    "                \"id\": 1,\n",
    "                \"name\": \"arima_global\",\n",
    "                \"type\": \"ARIMA\",\n",
    "                \"version\": \"v1.0.0\",\n",
    "                \"is_active\": True,\n",
    "                \"is_global\": True,\n",
    "                \"user_id\": None,\n",
    "                \"accuracy\": 0.87,\n",
    "                \"file_path\": \"models/arima/v1.0.0\",\n",
    "                \"trained_at\": \"2023-01-01T00:00:00Z\",\n",
    "                \"created_at\": \"2023-01-01T00:00:00Z\"\n",
    "            }\n",
    "        else:\n",
    "            raise HTTPException(\n",
    "                status_code=status.HTTP_404_NOT_FOUND,\n",
    "                detail=f\"Model with id {model_id} not found\"\n",
    "            )\n",
    "    except HTTPException as e:\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        # Log the error\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "            detail=f\"Error getting model details: {str(e)}\"\n",
    "        )\n",
    "\n",
    "@router.get(\"/{model_id}/metrics\", response_model=ModelMetrics)\n",
    "async def get_model_metrics(\n",
    "    model_id: int,\n",
    "    api_key: str = Depends(get_api_key),\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \\\"\\\"\\\"Get performance metrics for a specific model\\\"\\\"\\\"\n",
    "    try:\n",
    "        # Here we would fetch model metrics from MLflow\n",
    "        # This is a placeholder implementation\n",
    "        if model_id == 1:\n",
    "            return {\n",
    "                \"id\": 1,\n",
    "                \"model_id\": 1,\n",
    "                \"mae\": 1.2,\n",
    "                \"rmse\": 1.8,\n",
    "                \"accuracy\": 0.87,\n",
    "                \"sample_size\": 1000,\n",
    "                \"created_at\": \"2023-01-01T00:00:00Z\"\n",
    "            }\n",
    "        else:\n",
    "            raise HTTPException(\n",
    "                status_code=status.HTTP_404_NOT_FOUND,\n",
    "                detail=f\"Model with id {model_id} not found\"\n",
    "            )\n",
    "    except HTTPException as e:\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        # Log the error\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "            detail=f\"Error getting model metrics: {str(e)}\"\n",
    "        )\n",
    "\n",
    "@router.put(\"/{model_id}/activate\", response_model=ModelInfo)\n",
    "async def activate_model(\n",
    "    model_id: int,\n",
    "    api_key: str = Depends(get_api_key),\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \\\"\\\"\\\"Activate a specific model version\\\"\\\"\\\"\n",
    "    try:\n",
    "        # Here we would activate the model in MLflow\n",
    "        # This is a placeholder implementation\n",
    "        if model_id == 1 or model_id == 2:\n",
    "            return {\n",
    "                \"id\": model_id,\n",
    "                \"name\": \"arima_global\" if model_id == 1 else \"rf_global\",\n",
    "                \"type\": \"ARIMA\" if model_id == 1 else \"RandomForest\",\n",
    "                \"version\": \"v1.0.0\",\n",
    "                \"is_active\": True,\n",
    "                \"is_global\": True,\n",
    "                \"user_id\": None,\n",
    "                \"accuracy\": 0.87 if model_id == 1 else 0.85,\n",
    "                \"file_path\": f\"models/{'arima' if model_id == 1 else 'rf'}/v1.0.0\",\n",
    "                \"trained_at\": \"2023-01-01T00:00:00Z\",\n",
    "                \"created_at\": \"2023-01-01T00:00:00Z\"\n",
    "            }\n",
    "        else:\n",
    "            raise HTTPException(\n",
    "                status_code=status.HTTP_404_NOT_FOUND,\n",
    "                detail=f\"Model with id {model_id} not found\"\n",
    "            )\n",
    "    except HTTPException as e:\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        # Log the error\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "            detail=f\"Error activating model: {str(e)}\"\n",
    "        )\n",
    "\"\"\"\n",
    "    \n",
    "    with open(ml_engine_dir / \"app\" / \"api\" / \"v1\" / \"endpoints\" / \"models.py\", \"w\") as f:\n",
    "        f.write(models_content)\n",
    "    \n",
    "    print(f\"Created API router files in {ml_engine_dir / 'app' / 'api' / 'v1'}\")\n",
    "\n",
    "# Create schema definitions\n",
    "def create_schema_files(ml_engine_dir):\n",
    "    \"\"\"Create Pydantic schema files for data validation\"\"\"\n",
    "    # Create prediction.py schema\n",
    "    prediction_schema_content = \"\"\"from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Dict, Any\n",
    "from datetime import date, datetime\n",
    "\n",
    "# Period Prediction\n",
    "class PeriodPredictionRequest(BaseModel):\n",
    "    user_id: int\n",
    "    historical_cycles: List[Dict[str, Any]]\n",
    "    current_symptoms: Optional[List[Dict[str, Any]]] = []\n",
    "    last_period_start: date\n",
    "    average_cycle_length: Optional[int] = None\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"user_id\": 1,\n",
    "                \"historical_cycles\": [\n",
    "                    {\"start_date\": \"2023-01-01\", \"end_date\": \"2023-01-05\", \"duration\": 5},\n",
    "                    {\"start_date\": \"2023-01-28\", \"end_date\": \"2023-02-02\", \"duration\": 6}\n",
    "                ],\n",
    "                \"current_symptoms\": [\n",
    "                    {\"symptom_id\": 1, \"severity\": 3, \"date\": \"2023-02-25\"}\n",
    "                ],\n",
    "                \"last_period_start\": \"2023-01-28\",\n",
    "                \"average_cycle_length\": 28\n",
    "            }\n",
    "        }\n",
    "\n",
    "class PeriodPredictionResponse(BaseModel):\n",
    "    next_period_date: date\n",
    "    confidence_score: float\n",
    "    predicted_duration: int\n",
    "    window_start: date\n",
    "    window_end: date\n",
    "    model_version: str\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"next_period_date\": \"2023-02-25\",\n",
    "                \"confidence_score\": 0.88,\n",
    "                \"predicted_duration\": 5,\n",
    "                \"window_start\": \"2023-02-24\",\n",
    "                \"window_end\": \"2023-02-26\",\n",
    "                \"model_version\": \"v1.0.0\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Fertility Prediction\n",
    "class FertilityPredictionRequest(BaseModel):\n",
    "    user_id: int\n",
    "    historical_cycles: List[Dict[str, Any]]\n",
    "    basal_temperatures: Optional[List[Dict[str, Any]]] = []\n",
    "    cervical_fluid: Optional[List[Dict[str, Any]]] = []\n",
    "    ovulation_tests: Optional[List[Dict[str, Any]]] = []\n",
    "    last_period_start: date\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"user_id\": 1,\n",
    "                \"historical_cycles\": [\n",
    "                    {\"start_date\": \"2023-01-01\", \"end_date\": \"2023-01-05\", \"duration\": 5},\n",
    "                    {\"start_date\": \"2023-01-28\", \"end_date\": \"2023-02-02\", \"duration\": 6}\n",
    "                ],\n",
    "                \"basal_temperatures\": [\n",
    "                    {\"date\": \"2023-02-10\", \"value\": 36.5},\n",
    "                    {\"date\": \"2023-02-11\", \"value\": 36.7}\n",
    "                ],\n",
    "                \"last_period_start\": \"2023-01-28\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "class FertilityPredictionResponse(BaseModel):\n",
    "    fertility_window_start: date\n",
    "    fertility_window_end: date\n",
    "    peak_fertility_date: date\n",
    "    confidence_score: float\n",
    "    model_version: str\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"fertility_window_start\": \"2023-02-08\",\n",
    "                \"fertility_window_end\": \"2023-02-13\",\n",
    "                \"peak_fertility_date\": \"2023-02-11\",\n",
    "                \"confidence_score\": 0.82,\n",
    "                \"model_version\": \"v1.0.0\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Ovulation Prediction\n",
    "class OvulationPredictionRequest(BaseModel):\n",
    "    user_id: int\n",
    "    historical_cycles: List[Dict[str, Any]]\n",
    "    basal_temperatures: Optional[List[Dict[str, Any]]] = []\n",
    "    cervical_fluid: Optional[List[Dict[str, Any]]] = []\n",
    "    ovulation_tests: Optional[List[Dict[str, Any]]] = []\n",
    "    last_period_start: date\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"user_id\": 1,\n",
    "                \"historical_cycles\": [\n",
    "                    {\"start_date\": \"2023-01-01\", \"end_date\": \"2023-01-05\", \"duration\": 5},\n",
    "                    {\"start_date\": \"2023-01-28\", \"end_date\": \"2023-02-02\", \"duration\": 6}\n",
    "                ],\n",
    "                \"basal_temperatures\": [\n",
    "                    {\"date\": \"2023-02-10\", \"value\": 36.5},\n",
    "                    {\"date\": \"2023-02-11\", \"value\": 36.7}\n",
    "                ],\n",
    "                \"last_period_start\": \"2023-01-28\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "class OvulationPredictionResponse(BaseModel):\n",
    "    ovulation_date: date\n",
    "    confidence_score: float\n",
    "    model_version: str\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"ovulation_date\": \"2023-02-11\",\n",
    "                \"confidence_score\": 0.85,\n",
    "                \"model_version\": \"v1.0.0\"\n",
    "            }\n",
    "        }\n",
    "\"\"\"\n",
    "    \n",
    "    with open(ml_engine_dir / \"app\" / \"schemas\" / \"prediction.py\", \"w\") as f:\n",
    "        f.write(prediction_schema_content)\n",
    "    \n",
    "    # Create training.py schema\n",
    "    training_schema_content = \"\"\"from pydantic import BaseModel, Field\n",
    "from typing import Optional, List\n",
    "from datetime import datetime\n",
    "\n",
    "class TrainingRequest(BaseModel):\n",
    "    model_type: str = Field(..., description=\"Type of model to train: 'period', 'fertility', 'ovulation', 'all'\")\n",
    "    force: Optional[bool] = False\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"model_type\": \"period\",\n",
    "                \"force\": False\n",
    "            }\n",
    "        }\n",
    "\n",
    "class TrainingResponse(BaseModel):\n",
    "    status: str\n",
    "    model_type: str\n",
    "    user_id: Optional[int] = None\n",
    "    message: str\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"status\": \"training_started\",\n",
    "                \"model_type\": \"period\",\n",
    "                \"message\": \"Global period model training initiated\"\n",
    "            }\n",
    "        }\n",
    "\"\"\"\n",
    "    \n",
    "    with open(ml_engine_dir / \"app\" / \"schemas\" / \"training.py\", \"w\") as f:\n",
    "        f.write(training_schema_content)\n",
    "    \n",
    "    # Create ml_model.py schema\n",
    "    ml_model_schema_content = \"\"\"from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "class ModelInfo(BaseModel):\n",
    "    id: int\n",
    "    name: str\n",
    "    type: str\n",
    "    version: str\n",
    "    is_active: bool\n",
    "    is_global: bool\n",
    "    user_id: Optional[int] = None\n",
    "    accuracy: float\n",
    "    file_path: str\n",
    "    trained_at: datetime\n",
    "    created_at: datetime\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"id\": 1,\n",
    "                \"name\": \"arima_global\",\n",
    "                \"type\": \"ARIMA\",\n",
    "                \"version\": \"v1.0.0\",\n",
    "                \"is_active\": True,\n",
    "                \"is_global\": True,\n",
    "                \"user_id\": None,\n",
    "                \"accuracy\": 0.87,\n",
    "                \"file_path\": \"models/arima/v1.0.0\",\n",
    "                \"trained_at\": \"2023-01-01T00:00:00Z\",\n",
    "                \"created_at\": \"2023-01-01T00:00:00Z\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "class ModelMetrics(BaseModel):\n",
    "    id: int\n",
    "    model_id: int\n",
    "    mae: float\n",
    "    rmse: float\n",
    "    accuracy: float\n",
    "    sample_size: int\n",
    "    created_at: datetime\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"id\": 1,\n",
    "                \"model_id\": 1,\n",
    "                \"mae\": 1.2,\n",
    "                \"rmse\": 1.8,\n",
    "                \"accuracy\": 0.87,\n",
    "                \"sample_size\": 1000,\n",
    "                \"created_at\": \"2023-01-01T00:00:00Z\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "class ModelList(BaseModel):\n",
    "    models: List[ModelInfo]\n",
    "\"\"\"\n",
    "    \n",
    "    with open(ml_engine_dir / \"app\" / \"schemas\" / \"ml_model.py\", \"w\") as f:\n",
    "        f.write(ml_model_schema_content)\n",
    "    \n",
    "    print(f\"Created schema files in {ml_engine_dir / 'app' / 'schemas'}\")\n",
    "\n",
    "# Create database session management\n",
    "def create_db_files(ml_engine_dir):\n",
    "    \"\"\"Create database session management files\"\"\"\n",
    "    # Create session.py\n",
    "    session_content = \"\"\"from sqlalchemy import create_engine\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "from app.core.config import settings\n",
    "\n",
    "engine = create_engine(str(settings.SQLALCHEMY_DATABASE_URI))\n",
    "SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
    "\n",
    "Base = declarative_base()\n",
    "\"\"\"\n",
    "    \n",
    "    with open(ml_engine_dir / \"app\" / \"db\" / \"session.py\", \"w\") as f:\n",
    "        f.write(session_content)\n",
    "    \n",
    "    print(f\"Created database session files in {ml_engine_dir / 'app' / 'db'}\")\n",
    "\n",
    "# Create model component files with placeholder implementations\n",
    "def create_model_component_files(ml_engine_dir):\n",
    "    \"\"\"Create placeholder files for model components\"\"\"\n",
    "    # Create prediction_service.py in ensemble directory\n",
    "    prediction_service_content = \"\"\"from typing import Dict, Any, List\n",
    "from datetime import date, datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from app.models.arima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d226ca84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T15:54:28.286612Z",
     "iopub.status.busy": "2025-03-25T15:54:28.286356Z",
     "iopub.status.idle": "2025-03-25T15:54:28.314877Z",
     "shell.execute_reply": "2025-03-25T15:54:28.313991Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 1) (1213077052.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    I'll analyze the previous implementation and focus specifically on the issue with the `create_model_component_files()` function.\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 1)\n"
     ]
    }
   ],
   "source": [
    "I'll analyze the previous implementation and focus specifically on the issue with the `create_model_component_files()` function.\n",
    "\n",
    "[reflection on previous impl]\n",
    "In my previous implementation, I was creating a modular FastAPI application structure for the period tracker ML engine. I successfully created the directory structure, main.py, configuration files, API routers, schemas, and database utilities. However, when I tried to create the model component files, I encountered an error with the `prediction_service_content` variable in the `create_model_component_files()` function.\n",
    "\n",
    "The issue was that I had an unterminated triple-quoted string while defining the `predict_period` method in the `PredictionService` class. The string was cut off at `avg_cycle_length = data.get(\"average` without completing it properly. This caused a SyntaxError since the triple-quoted string wasn't properly closed.\n",
    "\n",
    "The problem is that I need to implement the model component files with complete definitions. I need to fix the incomplete string and provide proper implementations for all model components.\n",
    "\n",
    "[improved impl]:\n",
    "```python\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from metagpt.tools.libs.terminal import Terminal\n",
    "\n",
    "# Install required packages\n",
    "terminal = Terminal()\n",
    "# Uncomment to install required packages if they're missing\n",
    "# await terminal.run('pip install fastapi uvicorn sqlalchemy pydantic mlflow statsmodels scikit-learn')\n",
    "\n",
    "# Function to finalize the setup by creating the model component files\n",
    "def create_model_component_files(ml_engine_dir):\n",
    "    \"\"\"Create placeholder files for model components\"\"\"\n",
    "    \n",
    "    # Create ARIMA model file\n",
    "    arima_model_content = \"\"\"from typing import Dict, Any\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, datetime, timedelta\n",
    "import statsmodels.api as sm\n",
    "import mlflow\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "class ARIMAModel:\n",
    "    \"\"\"\n",
    "    ARIMA Time Series Model for period prediction\n",
    "    \"\"\"\n",
    "    def __init__(self, order=(1,0,0), seasonal_order=None):\n",
    "        self.order = order\n",
    "        self.seasonal_order = seasonal_order\n",
    "        self.model = None\n",
    "        self.version = \"v1.0.0\"\n",
    "        \n",
    "    def fit(self, data: pd.DataFrame) -> bool:\n",
    "        \"\"\"\n",
    "        Fit the ARIMA model to historical cycle data\n",
    "        \n",
    "        Args:\n",
    "            data: DataFrame with cycle history, must contain 'cycle_length' column\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if training succeeded\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract cycle lengths from data\n",
    "            cycle_lengths = data['cycle_length'].astype(float).values\n",
    "            \n",
    "            # Fit ARIMA model\n",
    "            self.model = sm.tsa.ARIMA(\n",
    "                cycle_lengths, \n",
    "                order=self.order\n",
    "            ).fit()\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error fitting ARIMA model: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def predict(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate period predictions based on ARIMA model\n",
    "        \n",
    "        Args:\n",
    "            data: Dict containing user's cycle history\n",
    "            \n",
    "        Returns:\n",
    "            Dict with period predictions\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # For simplicity in this placeholder, use average cycle length if model is not available\n",
    "            if self.model is None:\n",
    "                # Get historical cycles from data\n",
    "                historical_cycles = data.get(\"historical_cycles\", [])\n",
    "                \n",
    "                # Calculate average cycle length\n",
    "                if historical_cycles:\n",
    "                    cycle_lengths = [cycle.get(\"duration\", 28) for cycle in historical_cycles]\n",
    "                    avg_cycle_length = sum(cycle_lengths) / len(cycle_lengths)\n",
    "                else:\n",
    "                    avg_cycle_length = data.get(\"average_cycle_length\", 28)\n",
    "                \n",
    "                # Use last period start date to predict next one\n",
    "                last_period_start = data.get(\"last_period_start\")\n",
    "                if isinstance(last_period_start, str):\n",
    "                    last_period_start = datetime.strptime(last_period_start, \"%Y-%m-%d\").date()\n",
    "                \n",
    "                next_period_date = last_period_start + timedelta(days=int(avg_cycle_length))\n",
    "                confidence_score = 0.6  # Lower confidence without trained model\n",
    "            else:\n",
    "                # With a trained model, we would use the ARIMA prediction\n",
    "                # This is a placeholder for the actual implementation\n",
    "                forecast = self.model.forecast(steps=1)\n",
    "                predicted_cycle_length = int(forecast[0])\n",
    "                \n",
    "                # Use last period start date to predict next one\n",
    "                last_period_start = data.get(\"last_period_start\")\n",
    "                if isinstance(last_period_start, str):\n",
    "                    last_period_start = datetime.strptime(last_period_start, \"%Y-%m-%d\").date()\n",
    "                \n",
    "                next_period_date = last_period_start + timedelta(days=predicted_cycle_length)\n",
    "                confidence_score = 0.87  # Typical ARIMA model confidence\n",
    "            \n",
    "            # Calculate window based on confidence (±1 day for high confidence, ±2 for lower)\n",
    "            window_days = 1 if confidence_score > 0.8 else 2\n",
    "            window_start = next_period_date - timedelta(days=window_days)\n",
    "            window_end = next_period_date + timedelta(days=window_days)\n",
    "            \n",
    "            # Generate response\n",
    "            return {\n",
    "                \"next_period_date\": next_period_date,\n",
    "                \"confidence_score\": confidence_score,\n",
    "                \"predicted_duration\": 5,  # Placeholder - would be based on historical average\n",
    "                \"window_start\": window_start,\n",
    "                \"window_end\": window_end,\n",
    "                \"model_version\": self.version\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in ARIMA prediction: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def evaluate(self, data: pd.DataFrame) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate model performance on test data\n",
    "        \n",
    "        Args:\n",
    "            data: DataFrame with actual cycle data for evaluation\n",
    "            \n",
    "        Returns:\n",
    "            Dict with performance metrics\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Placeholder for evaluation logic\n",
    "            # In a real implementation, we'd calculate error metrics\n",
    "            return {\n",
    "                \"mae\": 1.2,  # Mean Absolute Error\n",
    "                \"rmse\": 1.8,  # Root Mean Squared Error\n",
    "                \"accuracy\": 0.87  # Accuracy within ±1 day\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating ARIMA model: {str(e)}\")\n",
    "            return {\"error\": str(e)}\n",
    "    \n",
    "    def save(self, path: str) -> str:\n",
    "        \"\"\"Save the model to disk\"\"\"\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "            with open(path, 'wb') as f:\n",
    "                pickle.dump(self.model, f)\n",
    "            return path\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving model: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def load(self, path: str) -> bool:\n",
    "        \"\"\"Load the model from disk\"\"\"\n",
    "        try:\n",
    "            with open(path, 'rb') as f:\n",
    "                self.model = pickle.load(f)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {str(e)}\")\n",
    "            return False\n",
    "\"\"\"\n",
    "\n",
    "    with open(ml_engine_dir / \"app\" / \"models\" / \"arima\" / \"arima_model.py\", \"w\") as f:\n",
    "        f.write(arima_model_content)\n",
    "\n",
    "    # Create Random Forest model file\n",
    "    rf_model_content = \"\"\"from typing import Dict, Any, List\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, datetime, timedelta\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "class RandomForestModel:\n",
    "    \"\"\"\n",
    "    Random Forest Regressor for symptom-based cycle predictions\n",
    "    \"\"\"\n",
    "    def __init__(self, n_estimators=100, max_depth=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.model = RandomForestRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            random_state=42\n",
    "        )\n",
    "        self.features = []\n",
    "        self.version = \"v1.0.0\"\n",
    "    \n",
    "    def fit(self, data: pd.DataFrame) -> bool:\n",
    "        \"\"\"\n",
    "        Train the Random Forest model on cycle and symptom data\n",
    "        \n",
    "        Args:\n",
    "            data: DataFrame with features (symptoms, lifestyle) and target (cycle shifts)\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if training succeeded\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract features and target from data\n",
    "            # This is a placeholder - actual implementation would process real data\n",
    "            X = data.drop('cycle_shift_days', axis=1)\n",
    "            y = data['cycle_shift_days']\n",
    "            \n",
    "            # Store feature names\n",
    "            self.features = X.columns.tolist()\n",
    "            \n",
    "            # Train the model\n",
    "            self.model.fit(X, y)\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error fitting Random Forest model: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def predict(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Predict cycle shifts based on symptoms\n",
    "        \n",
    "        Args:\n",
    "            data: Dict containing user's cycle history and symptoms\n",
    "            \n",
    "        Returns:\n",
    "            Dict with symptom-adjusted predictions\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # In a real implementation, we'd process the input data\n",
    "            # into a feature vector for prediction\n",
    "            # This is a placeholder implementation\n",
    "            \n",
    "            # Get base prediction (e.g., from ARIMA model or average)\n",
    "            last_period_start = data.get(\"last_period_start\")\n",
    "            if isinstance(last_period_start, str):\n",
    "                last_period_start = datetime.strptime(last_period_start, \"%Y-%m-%d\").date()\n",
    "            \n",
    "            # Get cycle length (default 28 if not available)\n",
    "            historical_cycles = data.get(\"historical_cycles\", [])\n",
    "            if historical_cycles:\n",
    "                cycle_lengths = [cycle.get(\"duration\", 28) for cycle in historical_cycles]\n",
    "                avg_cycle_length = sum(cycle_lengths) / len(cycle_lengths)\n",
    "            else:\n",
    "                avg_cycle_length = data.get(\"average_cycle_length\", 28)\n",
    "            \n",
    "            # Base prediction without symptoms\n",
    "            base_next_period = last_period_start + timedelta(days=int(avg_cycle_length))\n",
    "            \n",
    "            # If we have a trained model and symptoms, adjust the prediction\n",
    "            if self.model is not None and data.get(\"current_symptoms\"):\n",
    "                # In practice, we'd transform the symptoms into a feature vector\n",
    "                # matching the model's training features\n",
    "                # For this placeholder, just simulate a small shift\n",
    "                shift_days = 1 if len(data[\"current_symptoms\"]) > 0 else 0\n",
    "                adjusted_next_period = base_next_period + timedelta(days=shift_days)\n",
    "                confidence_score = 0.85\n",
    "            else:\n",
    "                # Without symptoms or model, use the base prediction\n",
    "                adjusted_next_period = base_next_period\n",
    "                confidence_score = 0.75\n",
    "            \n",
    "            # Generate response\n",
    "            return {\n",
    "                \"next_period_date\": adjusted_next_period,\n",
    "                \"confidence_score\": confidence_score,\n",
    "                \"predicted_duration\": 5,  # Placeholder\n",
    "                \"window_start\": adjusted_next_period - timedelta(days=1),\n",
    "                \"window_end\": adjusted_next_period + timedelta(days=1),\n",
    "                \"model_version\": self.version\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in Random Forest prediction: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def feature_importance(self) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Get feature importance from the trained model\n",
    "        \n",
    "        Returns:\n",
    "            Dict mapping feature names to importance scores\n",
    "        \"\"\"\n",
    "        if self.model is None or not self.features:\n",
    "            return {}\n",
    "        \n",
    "        importances = self.model.feature_importances_\n",
    "        return {feature: float(importance) for feature, importance in zip(self.features, importances)}\n",
    "    \n",
    "    def evaluate(self, data: pd.DataFrame) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate model performance on test data\n",
    "        \n",
    "        Args:\n",
    "            data: DataFrame with features and actual targets\n",
    "            \n",
    "        Returns:\n",
    "            Dict with performance metrics\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Placeholder for evaluation logic\n",
    "            # In a real implementation, we'd calculate prediction metrics\n",
    "            return {\n",
    "                \"mae\": 1.4,\n",
    "                \"rmse\": 2.0,\n",
    "                \"accuracy\": 0.85\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating Random Forest model: {str(e)}\")\n",
    "            return {\"error\": str(e)}\n",
    "    \n",
    "    def save(self, path: str) -> str:\n",
    "        \"\"\"Save the model to disk\"\"\"\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "            model_data = {\n",
    "                \"model\": self.model,\n",
    "                \"features\": self.features,\n",
    "                \"version\": self.version\n",
    "            }\n",
    "            with open(path, 'wb') as f:\n",
    "                pickle.dump(model_data, f)\n",
    "            return path\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving model: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def load(self, path: str) -> bool:\n",
    "        \"\"\"Load the model from disk\"\"\"\n",
    "        try:\n",
    "            with open(path, 'rb') as f:\n",
    "                model_data = pickle.load(f)\n",
    "                self.model = model_data[\"model\"]\n",
    "                self.features = model_data[\"features\"]\n",
    "                self.version = model_data.get(\"version\", \"v1.0.0\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {str(e)}\")\n",
    "            return False\n",
    "\"\"\"\n",
    "\n",
    "    with open(ml_engine_dir / \"app\" / \"models\" / \"random_forest\" / \"rf_model.py\", \"w\") as f:\n",
    "        f.write(rf_model_content)\n",
    "\n",
    "    # Create Gradient Boosting model file\n",
    "    gb_model_content = \"\"\"from typing import Dict, Any, List\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, datetime, timedelta\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "class GradientBoostingModel:\n",
    "    \"\"\"\n",
    "    Gradient Boosting Regressor for fertility and ovulation prediction\n",
    "    \"\"\"\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.model = GradientBoostingRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            max_depth=max_depth,\n",
    "            random_state=42\n",
    "        )\n",
    "        self.features = []\n",
    "        self.version = \"v1.0.0\"\n",
    "    \n",
    "    def fit(self, data: pd.DataFrame) -> bool:\n",
    "        \"\"\"\n",
    "        Train the Gradient Boosting model on fertility data\n",
    "        \n",
    "        Args:\n",
    "            data: DataFrame with features and target (days to ovulation)\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if training succeeded\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract features and target from data\n",
    "            # This is a placeholder - actual implementation would process real data\n",
    "            X = data.drop('days_to_ovulation', axis=1)\n",
    "            y = data['days_to_ovulation']\n",
    "            \n",
    "            # Store feature names\n",
    "            self.features = X.columns.tolist()\n",
    "            \n",
    "            # Train the model\n",
    "            self.model.fit(X, y)\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error fitting Gradient Boosting model: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def predict_fertility(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Predict fertility window based on user data\n",
    "        \n",
    "        Args:\n",
    "            data: Dict containing user's cycle data and fertility indicators\n",
    "            \n",
    "        Returns:\n",
    "            Dict with fertility window prediction\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get last period date\n",
    "            last_period_start = data.get(\"last_period_start\")\n",
    "            if isinstance(last_period_start, str):\n",
    "                last_period_start = datetime.strptime(last_period_start, \"%Y-%m-%d\").date()\n",
    "            \n",
    "            # Get cycle length (default 28 if not available)\n",
    "            historical_cycles = data.get(\"historical_cycles\", [])\n",
    "            if historical_cycles:\n",
    "                cycle_lengths = [cycle.get(\"duration\", 28) for cycle in historical_cycles]\n",
    "                avg_cycle_length = sum(cycle_lengths) / len(cycle_lengths)\n",
    "            else:\n",
    "                avg_cycle_length = data.get(\"average_cycle_length\", 28)\n",
    "            \n",
    "            # Standard fertility window calculation (if no model or insufficient data)\n",
    "            # Typically days 10-17 of a 28-day cycle, adjusted proportionally\n",
    "            if self.model is None:\n",
    "                # Calculate ovulation day (approx. 14 days before next period in a 28-day cycle)\n",
    "                cycle_proportion = avg_cycle_length / 28\n",
    "                ovulation_day = int(14 * cycle_proportion)\n",
    "                fertility_start_day = int(10 * cycle_proportion)\n",
    "                fertility_end_day = int(17 * cycle_proportion)\n",
    "                \n",
    "                ovulation_date = last_period_start + timedelta(days=ovulation_day)\n",
    "                fertility_window_start = last_period_start + timedelta(days=fertility_start_day)\n",
    "                fertility_window_end = last_period_start + timedelta(days=fertility_end_day)\n",
    "                confidence_score = 0.7  # Lower without a trained model\n",
    "            else:\n",
    "                # With a trained model, we'd use features like BBT, cervical fluid, etc.\n",
    "                # This is a placeholder implementation\n",
    "                if data.get(\"basal_temperatures\") or data.get(\"cervical_fluid\"):\n",
    "                    # Simulate slight adjustment based on additional indicators\n",
    "                    ovulation_day = int(avg_cycle_length / 2) - 1\n",
    "                    fertility_start_day = ovulation_day - 5\n",
    "                    fertility_end_day = ovulation_day + 1\n",
    "                    \n",
    "                    ovulation_date = last_period_start + timedelta(days=ovulation_day)\n",
    "                    fertility_window_start = last_period_start + timedelta(days=fertility_start_day)\n",
    "                    fertility_window_end = last_period_start + timedelta(days=fertility_end_day)\n",
    "                    confidence_score = 0.82\n",
    "                else:\n",
    "                    # Fall back to standard calculation\n",
    "                    ovulation_day = int(avg_cycle_length / 2)\n",
    "                    fertility_start_day = ovulation_day - 5\n",
    "                    fertility_end_day = ovulation_day + 1\n",
    "                    \n",
    "                    ovulation_date = last_period_start + timedelta(days=ovulation_day)\n",
    "                    fertility_window_start = last_period_start + timedelta(days=fertility_start_day)\n",
    "                    fertility_window_end = last_period_start + timedelta(days=fertility_end_day)\n",
    "                    confidence_score = 0.75\n",
    "            \n",
    "            # Generate response\n",
    "            return {\n",
    "                \"fertility_window_start\": fertility_window_start,\n",
    "                \"fertility_window_end\": fertility_window_end,\n",
    "                \"peak_fertility_date\": ovulation_date,\n",
    "                \"confidence_score\": confidence_score,\n",
    "                \"model_version\": self.version\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in fertility prediction: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def predict_ovulation(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Predict ovulation day based on user data\n",
    "        \n",
    "        Args:\n",
    "            data: Dict containing user's cycle data and fertility indicators\n",
    "            \n",
    "        Returns:\n",
    "            Dict with ovulation prediction\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # For simplicity, we'll reuse the fertility prediction logic\n",
    "            fertility_prediction = self.predict_fertility(data)\n",
    "            \n",
    "            # Return just the ovulation-specific information\n",
    "            return {\n",
    "                \"ovulation_date\": fertility_prediction[\"peak_fertility_date\"],\n",
    "                \"confidence_score\": fertility_prediction[\"confidence_score\"],\n",
    "                \"model_version\": self.version\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in ovulation prediction: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def feature_importance(self) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Get feature importance from the trained model\n",
    "        \n",
    "        Returns:\n",
    "            Dict mapping feature names to importance scores\n",
    "        \"\"\"\n",
    "        if self.model is None or not self.features:\n",
    "            return {}\n",
    "        \n",
    "        importances = self.model.feature_importances_\n",
    "        return {feature: float(importance) for feature, importance in zip(self.features, importances)}\n",
    "    \n",
    "    def evaluate(self, data: pd.DataFrame) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate model performance on test data\n",
    "        \n",
    "        Args:\n",
    "            data: DataFrame with features and actual targets\n",
    "            \n",
    "        Returns:\n",
    "            Dict with performance metrics\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Placeholder for evaluation logic\n",
    "            # In a real implementation, we'd calculate prediction metrics\n",
    "            return {\n",
    "                \"mae\": 1.2,\n",
    "                \"rmse\": 1.6,\n",
    "                \"accuracy\": 0.82\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating Gradient Boosting model: {str(e)}\")\n",
    "            return {\"error\": str(e)}\n",
    "    \n",
    "    def save(self, path: str) -> str:\n",
    "        \"\"\"Save the model to disk\"\"\"\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "            model_data = {\n",
    "                \"model\": self.model,\n",
    "                \"features\": self.features,\n",
    "                \"version\": self.version\n",
    "            }\n",
    "            with open(path, 'wb') as f:\n",
    "                pickle.dump(model_data, f)\n",
    "            return path\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving model: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def load(self, path: str) -> bool:\n",
    "        \"\"\"Load the model from disk\"\"\"\n",
    "        try:\n",
    "            with open(path, 'rb') as f:\n",
    "                model_data = pickle.load(f)\n",
    "                self.model = model_data[\"model\"]\n",
    "                self.features = model_data[\"features\"]\n",
    "                self.version = model_data.get(\"version\", \"v1.0.0\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {str(e)}\")\n",
    "            return False\n",
    "\"\"\"\n",
    "\n",
    "    with open(ml_engine_dir / \"app\" / \"models\" / \"gradient_boosting\" / \"gb_model.py\", \"w\") as f:\n",
    "        f.write(gb_model_content)\n",
    "\n",
    "    # Create Ensemble Integrator file\n",
    "    ensemble_integrator_content = \"\"\"from typing import Dict, Any, List\n",
    "import numpy as np\n",
    "from datetime import date, timedelta\n",
    "\n",
    "class EnsembleIntegrator:\n",
    "    \"\"\"\n",
    "    Integrates predictions from multiple models using weighted averaging\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Initial weights for each model type\n",
    "        self.weights = {\n",
    "            \"arima\": 0.4,\n",
    "            \"random_forest\": 0.3,\n",
    "            \"gradient_boosting\": 0.3\n",
    "        }\n",
    "    \n",
    "    def integrate_predictions(self, predictions: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Combine predictions from multiple models\n",
    "        \n",
    "        Args:\n",
    "            predictions: List of prediction dictionaries from different models\n",
    "            \n",
    "        Returns:\n",
    "            Dict with integrated prediction\n",
    "        \"\"\"\n",
    "        if not predictions:\n",
    "            raise ValueError(\"No predictions provided\")\n",
    "        \n",
    "        # For date predictions (like next period date), use weighted voting\n",
    "        dates = []\n",
    "        weights = []\n",
    "        confidence_scores = []\n",
    "        \n",
    "        # Extract the relevant data from each prediction\n",
    "        for pred in predictions:\n",
    "            model_type = pred.get(\"model_type\", \"\")\n",
    "            confidence = pred.get(\"confidence_score\", 0.5)\n",
    "            \n",
    "            if \"next_period_date\" in pred:\n",
    "                dates.append(pred[\"next_period_date\"])\n",
    "                weights.append(self.weights.get(model_type, 0.33) * confidence)\n",
    "                confidence_scores.append(confidence)\n",
    "        \n",
    "        if not dates:\n",
    "            # If no date predictions available, use the first prediction as is\n",
    "            return predictions[0]\n",
    "        \n",
    "        # Implement a weighted date selection\n",
    "        # For simplicity, choose the date with the highest weight\n",
    "        best_idx = np.argmax(weights)\n",
    "        best_date = dates[best_idx]\n",
    "        \n",
    "        # For confidence, take weighted average of confidence scores\n",
    "        if confidence_scores:\n",
    "            avg_confidence = sum(confidence_scores) / len(confidence_scores)\n",
    "            # Boost confidence if models agree\n",
    "            date_range = set((date - dates[0]).days for date in dates)\n",
    "            if len(date_range) <= 2:  # Models agree within 2 days\n",
    "                avg_confidence = min(avg_confidence + 0.05, 0.95)\n",
    "        else:\n",
    "            avg_confidence = 0.7\n",
    "        \n",
    "        # Construct integrated prediction\n",
    "        # Use fields from the highest-weighted prediction as a base\n",
    "        integrated = predictions[best_idx].copy()\n",
    "        \n",
    "        # Override with ensemble-specific values\n",
    "        integrated[\"confidence_score\"] = avg_confidence\n",
    "        integrated[\"model_version\"] = \"ensemble-v1.0.0\"\n",
    "        \n",
    "        return integrated\n",
    "    \n",
    "    def adjust_weights(self, performance: Dict[str, Dict[str, float]]) -> bool:\n",
    "        \"\"\"\n",
    "        Adjust model weights based on performance metrics\n",
    "        \n",
    "        Args:\n",
    "            performance: Dict mapping model types to performance metrics\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if weights were adjusted\n",
    "        \"\"\"\n",
    "        if not performance:\n",
    "            return False\n",
    "        \n",
    "        # Extract accuracy from performance metrics\n",
    "        accuracies = {}\n",
    "        for model_type, metrics in performance.items():\n",
    "            if \"accuracy\" in metrics:\n",
    "                accuracies[model_type] = metrics[\"accuracy\"]\n",
    "        \n",
    "        if not accuracies:\n",
    "            return False\n",
    "        \n",
    "        # Normalize accuracies to get weights\n",
    "        total_accuracy = sum(accuracies.values())\n",
    "        if total_accuracy == 0:\n",
    "            return False\n",
    "            \n",
    "        new_weights = {model: acc/total_accuracy for model, acc in accuracies.items()}\n",
    "        \n",
    "        # Update weights\n",
    "        self.weights.update(new_weights)\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def get_confidence_score(self, predictions: List[Dict[str, Any]]) -> float:\n",
    "        \"\"\"\n",
    "        Calculate confidence score based on agreement among models\n",
    "        \n",
    "        Args:\n",
    "            predictions: List of prediction dictionaries from different models\n",
    "            \n",
    "        Returns:\n",
    "            float: Confidence score (0.0-1.0)\n",
    "        \"\"\"\n",
    "        if not predictions or len(predictions) < 2:\n",
    "            return 0.7  # Default confidence with insufficient data\n",
    "        \n",
    "        # Extract dates and confidence scores\n",
    "        dates = []\n",
    "        confidences = []\n",
    "        \n",
    "        for pred in predictions:\n",
    "            if \"next_period_date\" in pred:\n",
    "                dates.append(pred[\"next_period_date\"])\n",
    "                confidences.append(pred.get(\"confidence_score\", 0.5))\n",
    "        \n",
    "        if not dates:\n",
    "            return 0.7\n",
    "        \n",
    "        # Calculate date agreement (days between min and max prediction)\n",
    "        if len(dates) > 1:\n",
    "            date_range = (max(dates) - min(dates)).days\n",
    "            \n",
    "            # Higher agreement = higher confidence\n",
    "            agreement_factor = 1.0 if date_range == 0 else (1.0 / (date_range + 1))\n",
    "            \n",
    "            # Combine individual confidences with agreement\n",
    "            base_confidence = sum(confidences) / len(confidences)\n",
    "            ensemble_confidence = 0.7 * base_confidence + 0.3 * agreement_factor\n",
    "            \n",
    "            # Cap at 0.95 to acknowledge inherent uncertainty\n",
    "            return min(ensemble_confidence, 0.95)\n",
    "        else:\n",
    "            # Single prediction\n",
    "            return confidences[0] if confidences else 0.7\n",
    "\"\"\"\n",
    "\n",
    "    with open(ml_engine_dir / \"app\" / \"models\" / \"ensemble\" / \"ensemble_integrator.py\", \"w\") as f:\n",
    "        f.write(ensemble_integrator_content)\n",
    "\n",
    "    # Create prediction service file that was causing issues\n",
    "    prediction_service_content = \"\"\"from typing import Dict, Any, List\n",
    "from datetime import date, datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from app.models.arima.arima_model import ARIMAModel\n",
    "from app.models.random_forest.rf_model import RandomForestModel\n",
    "from app.models.gradient_boosting.gb_model import GradientBoostingModel\n",
    "from app.models.ensemble.ensemble_integrator import EnsembleIntegrator\n",
    "\n",
    "class PredictionService:\n",
    "    def __init__(self):\n",
    "        self.arima_model = ARIMAModel()\n",
    "        self.rf_model = RandomForestModel()\n",
    "        self.gb_model = GradientBoostingModel()\n",
    "        self.ensemble_integrator = EnsembleIntegrator()\n",
    "    \n",
    "    def predict_period(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate period predictions using ensemble model\n",
    "        \n",
    "        Args:\n",
    "            data: Dict containing user's cycle history and current status\n",
    "            \n",
    "        Returns:\n",
    "            Dict with period predictions and confidence score\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get model predictions\n",
    "            arima_pred = self.arima_model.predict(data)\n",
    "            arima_pred[\"model_type\"] = \"arima\"\n",
    "            \n",
    "            rf_pred = self.rf_model.predict(data)\n",
    "            rf_pred[\"model_type\"] = \"random_forest\"\n",
    "            \n",
    "            # Combine predictions with ensemble integrator\n",
    "            predictions = [arima_pred, rf_pred]\n",
    "            \n",
    "            # Integrate the predictions\n",
    "            result = self.ensemble_integrator.integrate_predictions(predictions)\n",
    "            \n",
    "            # Add version information\n",
    "            result[\"model_version\"] = \"ensemble-v1.0.0\"\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in period prediction: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def predict_fertility(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate fertility window predictions\n",
    "        \n",
    "        Args:\n",
    "            data: Dict containing user's cycle and fertility data\n",
    "            \n",
    "        Returns:\n",
    "            Dict with fertility window predictions\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # For fertility predictions, we primarily use the gradient boosting model\n",
    "            return self.gb_model.predict_fertility(data)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in fertility prediction: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def predict_ovulation(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate ovulation day predictions\n",
    "        \n",
    "        Args:\n",
    "            data: Dict containing user's cycle and fertility data\n",
    "            \n",
    "        Returns:\n",
    "            Dict with ovulation day prediction\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # For ovulation predictions, we use the gradient boosting model\n",
    "            return self.gb_model.predict_ovulation(data)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in ovulation prediction: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def get_confidence(self, prediction: Dict[str, Any]) -> float:\n",
    "        \"\"\"\n",
    "        Get confidence score for a prediction\n",
    "        \n",
    "        Args:\n",
    "            prediction: Dict containing prediction results\n",
    "            \n",
    "        Returns:\n",
    "            float: Confidence score (0.0-1.0)\n",
    "        \"\"\"\n",
    "        return prediction.get(\"confidence_score\", 0.7)\n",
    "\"\"\"\n",
    "\n",
    "    with open(ml_engine_dir / \"app\" / \"models\" / \"ensemble\" / \"prediction_service.py\", \"w\") as f:\n",
    "        f.write(prediction_service_content)\n",
    "        \n",
    "    # Create model trainer file\n",
    "    model_trainer_content = \"\"\"from typing import Dict, Any, Optional\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import logging\n",
    "\n",
    "from app.models.arima.arima_model import ARIMAModel\n",
    "from app.models.random_forest.rf_model import RandomForestModel\n",
    "from app.models.gradient_boosting.gb_model import GradientBoostingModel\n",
    "from app.models.ensemble.ensemble_integrator import EnsembleIntegrator\n",
    "from app.preprocessing.data_preprocessor import DataPreprocessor\n",
    "from app.core.config import settings\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self):\n",
    "        self.preprocessor = DataPreprocessor()\n",
    "        self.models_path = settings.MODEL_PATH\n",
    "        self.min_cycles = settings.MIN_CYCLES_FOR_USER_MODEL\n",
    "        \n",
    "        # Set up MLflow\n",
    "        mlflow.set_tracking_uri(settings.MLFLOW_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01267aeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T15:57:06.379478Z",
     "iopub.status.busy": "2025-03-25T15:57:06.378791Z",
     "iopub.status.idle": "2025-03-25T15:57:06.401819Z",
     "shell.execute_reply": "2025-03-25T15:57:06.400522Z"
    }
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (2023484187.py, line 148)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 148\u001b[0;36m\u001b[0m\n\u001b[0;31m    Get database session for dependency injection\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up directory paths\n",
    "ml_engine_dir = Path('/data/chats/90kuvd/workspace/period_tracker_ml')\n",
    "app_dir = ml_engine_dir / \"app\"\n",
    "\n",
    "# 1. Create the main FastAPI application file\n",
    "main_content = \"\"\"from fastapi import FastAPI, Depends, HTTPException, Security\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.security.api_key import APIKeyHeader, APIKey\n",
    "from starlette.status import HTTP_403_FORBIDDEN\n",
    "import logging\n",
    "import sys\n",
    "from typing import Dict, Any\n",
    "\n",
    "from app.api.v1.router import api_router\n",
    "from app.core.config import settings\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize FastAPI application\n",
    "app = FastAPI(\n",
    "    title=settings.PROJECT_NAME,\n",
    "    description=settings.PROJECT_DESCRIPTION,\n",
    "    version=settings.VERSION,\n",
    "    openapi_url=f\"{settings.API_V1_STR}/openapi.json\"\n",
    ")\n",
    "\n",
    "# API key authentication\n",
    "api_key_header = APIKeyHeader(name=settings.API_KEY_NAME, auto_error=False)\n",
    "\n",
    "async def get_api_key(api_key_header: str = Security(api_key_header)):\n",
    "    if api_key_header == settings.API_KEY:\n",
    "        return api_key_header\n",
    "    else:\n",
    "        raise HTTPException(\n",
    "            status_code=HTTP_403_FORBIDDEN, detail=\"Invalid API Key\"\n",
    "        )\n",
    "\n",
    "# Set up CORS middleware\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=settings.CORS_ORIGINS,\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Include API router\n",
    "app.include_router(\n",
    "    api_router,\n",
    "    prefix=settings.API_V1_STR,\n",
    "    dependencies=[Depends(get_api_key)] if settings.API_KEY_REQUIRED else []\n",
    ")\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    return {\"status\": \"healthy\", \"version\": settings.VERSION}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\"\"\"\n",
    "\n",
    "# 2. Create configuration file\n",
    "config_content = \"\"\"from typing import List, Optional, Union, Dict, Any\n",
    "from pydantic import BaseSettings, validator\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "class Settings(BaseSettings):\n",
    "    # API Settings\n",
    "    API_V1_STR: str = \"/api/v1\"\n",
    "    PROJECT_NAME: str = \"Period Tracker ML Engine\"\n",
    "    PROJECT_DESCRIPTION: str = \"ML Engine for the Period Tracker App\"\n",
    "    VERSION: str = \"0.1.0\"\n",
    "    \n",
    "    # Security\n",
    "    API_KEY_NAME: str = \"X-API-KEY\"\n",
    "    API_KEY: str = os.getenv(\"API_KEY\", \"development_api_key\")\n",
    "    API_KEY_REQUIRED: bool = True\n",
    "    \n",
    "    # CORS\n",
    "    CORS_ORIGINS: List[str] = [\"http://localhost:8000\", \"http://localhost:3000\"]\n",
    "    \n",
    "    # Database\n",
    "    POSTGRES_SERVER: str = os.getenv(\"POSTGRES_SERVER\", \"localhost\")\n",
    "    POSTGRES_USER: str = os.getenv(\"POSTGRES_USER\", \"postgres\")\n",
    "    POSTGRES_PASSWORD: str = os.getenv(\"POSTGRES_PASSWORD\", \"postgres\")\n",
    "    POSTGRES_DB: str = os.getenv(\"POSTGRES_DB\", \"period_tracker\")\n",
    "    SQLALCHEMY_DATABASE_URI: Optional[str] = None\n",
    "    \n",
    "    @validator(\"SQLALCHEMY_DATABASE_URI\", pre=True)\n",
    "    def assemble_db_connection(cls, v: Optional[str], values: Dict[str, Any]) -> str:\n",
    "        if v:\n",
    "            return v\n",
    "        return f\"postgresql://{values.get('POSTGRES_USER')}:{values.get('POSTGRES_PASSWORD')}@{values.get('POSTGRES_SERVER')}/{values.get('POSTGRES_DB')}\"\n",
    "    \n",
    "    # ML Settings\n",
    "    MODEL_PATH: str = os.getenv(\"MODEL_PATH\", str(Path(__file__).parent.parent.parent / \"models\"))\n",
    "    MLFLOW_TRACKING_URI: str = os.getenv(\"MLFLOW_TRACKING_URI\", \"sqlite:///mlflow.db\")\n",
    "    \n",
    "    # ML Parameters\n",
    "    MIN_CYCLES_FOR_TRAINING: int = 3\n",
    "    MIN_CYCLES_FOR_USER_MODEL: int = 6\n",
    "    RETRAINING_GLOBAL_SCHEDULE: str = \"0 0 * * 0\"  # Weekly on Sunday at midnight\n",
    "    RETRAINING_USER_SCHEDULE: str = \"0 0 1 * *\"    # Monthly on 1st at midnight\n",
    "    \n",
    "    # Performance Thresholds\n",
    "    MIN_ACCURACY_THRESHOLD: float = 0.85\n",
    "    MAX_ERROR_DAYS: int = 3\n",
    "    \n",
    "    class Config:\n",
    "        case_sensitive = True\n",
    "        env_file = \".env\"\n",
    "\n",
    "settings = Settings()\n",
    "\"\"\"\n",
    "\n",
    "# 3. Create database connection utilities\n",
    "db_content = \"\"\"from sqlalchemy import create_engine\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import pandas as pd\n",
    "from typing import Generator, Dict, Any, List\n",
    "import logging\n",
    "\n",
    "from app.core.config import settings\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Create SQLAlchemy engine and session\n",
    "engine = create_engine(settings.SQLALCHEMY_DATABASE_URI)\n",
    "SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
    "\n",
    "# Base class for database models\n",
    "Base = declarative_base()\n",
    "\n",
    "def get_db() -> Generator:\n",
    "    \"\"\"\n",
    "    Get database session for dependency injection\n",
    "    \"\"\"\n",
    "    db = SessionLocal()\n",
    "    try:\n",
    "        yield db\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "async def fetch_user_data(user_id: int) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fetch user data from the database for predictions\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create an SQL query to get user cycle data, symptoms, etc.\n",
    "        query = f'''\n",
    "        SELECT \n",
    "            c.id, \n",
    "            c.user_id,\n",
    "            c.start_date, \n",
    "            c.end_date, \n",
    "            c.duration, \n",
    "            c.notes\n",
    "        FROM \n",
    "            cycles c\n",
    "        WHERE \n",
    "            c.user_id = {user_id}\n",
    "        ORDER BY \n",
    "            c.start_date DESC\n",
    "        '''\n",
    "        \n",
    "        # Execute query and convert to pandas DataFrame\n",
    "        with engine.connect() as connection:\n",
    "            cycles_df = pd.read_sql(query, connection)\n",
    "        \n",
    "        # Query for user's symptoms\n",
    "        symptom_query = f'''\n",
    "        SELECT \n",
    "            sl.id,\n",
    "            sl.user_id,\n",
    "            sl.symptom_id,\n",
    "            sl.log_date,\n",
    "            sl.severity,\n",
    "            sl.notes,\n",
    "            s.name as symptom_name,\n",
    "            s.category\n",
    "        FROM \n",
    "            symptom_logs sl\n",
    "        JOIN\n",
    "            symptoms s ON sl.symptom_id = s.id\n",
    "        WHERE \n",
    "            sl.user_id = {user_id}\n",
    "        ORDER BY \n",
    "            sl.log_date DESC\n",
    "        '''\n",
    "        \n",
    "        with engine.connect() as connection:\n",
    "            symptoms_df = pd.read_sql(symptom_query, connection)\n",
    "        \n",
    "        # Query for biometric data\n",
    "        biometric_query = f'''\n",
    "        SELECT \n",
    "            id,\n",
    "            user_id,\n",
    "            type,\n",
    "            value,\n",
    "            log_date,\n",
    "            notes\n",
    "        FROM \n",
    "            biometric_logs\n",
    "        WHERE \n",
    "            user_id = {user_id}\n",
    "        ORDER BY \n",
    "            log_date DESC\n",
    "        '''\n",
    "        \n",
    "        with engine.connect() as connection:\n",
    "            biometrics_df = pd.read_sql(biometric_query, connection)\n",
    "        \n",
    "        # Process the data into the required format\n",
    "        user_data = {\n",
    "            \"user_id\": user_id,\n",
    "            \"historical_cycles\": cycles_df.to_dict('records'),\n",
    "            \"symptoms\": symptoms_df.to_dict('records'),\n",
    "            \"biometrics\": biometrics_df.to_dict('records'),\n",
    "        }\n",
    "        \n",
    "        # Calculate average cycle length if there's data\n",
    "        if not cycles_df.empty:\n",
    "            user_data[\"average_cycle_length\"] = cycles_df[\"duration\"].mean()\n",
    "            user_data[\"last_period_start\"] = cycles_df.iloc[0][\"start_date\"]\n",
    "        \n",
    "        return user_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching user data: {str(e)}\")\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "async def fetch_training_data(is_anonymous: bool = True) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Fetch data for model training from the database\n",
    "    \n",
    "    Args:\n",
    "        is_anonymous: If True, anonymizes the data\n",
    "        \n",
    "    Returns:\n",
    "        Dict with DataFrames for cycles, symptoms, biometrics, etc.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create SQL queries for the required data\n",
    "        cycles_query = '''\n",
    "        SELECT \n",
    "            c.id, \n",
    "            c.user_id,\n",
    "            c.start_date, \n",
    "            c.end_date, \n",
    "            c.duration\n",
    "        FROM \n",
    "            cycles c\n",
    "        ORDER BY \n",
    "            c.user_id, c.start_date\n",
    "        '''\n",
    "        \n",
    "        symptoms_query = '''\n",
    "        SELECT \n",
    "            sl.id,\n",
    "            sl.user_id,\n",
    "            sl.symptom_id,\n",
    "            sl.log_date,\n",
    "            sl.severity,\n",
    "            s.name as symptom_name,\n",
    "            s.category\n",
    "        FROM \n",
    "            symptom_logs sl\n",
    "        JOIN\n",
    "            symptoms s ON sl.symptom_id = s.id\n",
    "        ORDER BY \n",
    "            sl.user_id, sl.log_date\n",
    "        '''\n",
    "        \n",
    "        biometrics_query = '''\n",
    "        SELECT \n",
    "            id,\n",
    "            user_id,\n",
    "            type,\n",
    "            value,\n",
    "            log_date\n",
    "        FROM \n",
    "            biometric_logs\n",
    "        ORDER BY \n",
    "            user_id, log_date\n",
    "        '''\n",
    "        \n",
    "        # Execute queries and convert to pandas DataFrames\n",
    "        with engine.connect() as connection:\n",
    "            cycles_df = pd.read_sql(cycles_query, connection)\n",
    "            symptoms_df = pd.read_sql(symptoms_query, connection)\n",
    "            biometrics_df = pd.read_sql(biometrics_query, connection)\n",
    "        \n",
    "        # Anonymize data if requested\n",
    "        if is_anonymous:\n",
    "            # Replace user_id with anonymous identifiers\n",
    "            user_mapping = {}\n",
    "            for df in [cycles_df, symptoms_df, biometrics_df]:\n",
    "                if 'user_id' in df.columns:\n",
    "                    for user_id in df['user_id'].unique():\n",
    "                        if user_id not in user_mapping:\n",
    "                            user_mapping[user_id] = f\"anon_{len(user_mapping)+1}\"\n",
    "                    df['user_id'] = df['user_id'].map(user_mapping)\n",
    "        \n",
    "        return {\n",
    "            \"cycles\": cycles_df,\n",
    "            \"symptoms\": symptoms_df,\n",
    "            \"biometrics\": biometrics_df\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching training data: {str(e)}\")\n",
    "        return {\"error\": str(e)}\n",
    "\"\"\"\n",
    "\n",
    "# 4. Create API router for v1\n",
    "api_router_content = \"\"\"from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks\n",
    "from sqlalchemy.orm import Session\n",
    "from typing import List, Dict, Any, Optional\n",
    "import logging\n",
    "\n",
    "from app.db.database import get_db\n",
    "from app.schemas.prediction import (\n",
    "    PeriodPredictionRequest, \n",
    "    PeriodPredictionResponse, \n",
    "    FertilityPredictionRequest, \n",
    "    FertilityPredictionResponse,\n",
    "    OvulationPredictionRequest,\n",
    "    OvulationPredictionResponse,\n",
    "    PredictionFeedbackRequest,\n",
    "    PredictionFeedbackResponse\n",
    ")\n",
    "from app.schemas.training import (\n",
    "    TrainingRequest, \n",
    "    TrainingResponse, \n",
    "    ModelInfoResponse,\n",
    "    ModelMetricsResponse\n",
    ")\n",
    "from app.models.ensemble.prediction_service import PredictionService\n",
    "from app.training.model_trainer import ModelTrainer\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "api_router = APIRouter()\n",
    "prediction_service = PredictionService()\n",
    "model_trainer = ModelTrainer()\n",
    "\n",
    "# Prediction endpoints\n",
    "@api_router.post(\"/predictions/period\", response_model=PeriodPredictionResponse)\n",
    "async def predict_period(\n",
    "    request: PeriodPredictionRequest,\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \"\"\"Predict next period date\"\"\"\n",
    "    try:\n",
    "        # Prepare input data for prediction\n",
    "        prediction_data = request.dict()\n",
    "        \n",
    "        # If user_id is provided, fetch additional data from database\n",
    "        if request.user_id:\n",
    "            from app.db.database import fetch_user_data\n",
    "            user_data = await fetch_user_data(request.user_id)\n",
    "            prediction_data.update(user_data)\n",
    "        \n",
    "        # Generate prediction\n",
    "        prediction_result = prediction_service.predict_period(prediction_data)\n",
    "        \n",
    "        return prediction_result\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Period prediction error: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@api_router.post(\"/predictions/fertility\", response_model=FertilityPredictionResponse)\n",
    "async def predict_fertility(\n",
    "    request: FertilityPredictionRequest,\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \"\"\"Predict fertility window\"\"\"\n",
    "    try:\n",
    "        # Prepare input data for prediction\n",
    "        prediction_data = request.dict()\n",
    "        \n",
    "        # If user_id is provided, fetch additional data from database\n",
    "        if request.user_id:\n",
    "            from app.db.database import fetch_user_data\n",
    "            user_data = await fetch_user_data(request.user_id)\n",
    "            prediction_data.update(user_data)\n",
    "        \n",
    "        # Generate prediction\n",
    "        prediction_result = prediction_service.predict_fertility(prediction_data)\n",
    "        \n",
    "        return prediction_result\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fertility prediction error: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@api_router.post(\"/predictions/ovulation\", response_model=OvulationPredictionResponse)\n",
    "async def predict_ovulation(\n",
    "    request: OvulationPredictionRequest,\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \"\"\"Predict ovulation day\"\"\"\n",
    "    try:\n",
    "        # Prepare input data for prediction\n",
    "        prediction_data = request.dict()\n",
    "        \n",
    "        # If user_id is provided, fetch additional data from database\n",
    "        if request.user_id:\n",
    "            from app.db.database import fetch_user_data\n",
    "            user_data = await fetch_user_data(request.user_id)\n",
    "            prediction_data.update(user_data)\n",
    "        \n",
    "        # Generate prediction\n",
    "        prediction_result = prediction_service.predict_ovulation(prediction_data)\n",
    "        \n",
    "        return prediction_result\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Ovulation prediction error: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@api_router.post(\"/predictions/feedback\", response_model=PredictionFeedbackResponse)\n",
    "async def submit_feedback(\n",
    "    request: PredictionFeedbackRequest,\n",
    "    background_tasks: BackgroundTasks,\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \"\"\"Submit feedback on a prediction\"\"\"\n",
    "    try:\n",
    "        # Store the feedback in the database\n",
    "        # This would be implemented in a real system\n",
    "        \n",
    "        # Queue model evaluation if sufficient feedback is collected\n",
    "        background_tasks.add_task(\n",
    "            model_trainer.evaluate_model_performance, \n",
    "            user_id=request.user_id if request.user_id else None\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"message\": \"Feedback submitted successfully\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Feedback submission error: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# Model training and management endpoints\n",
    "@api_router.post(\"/models/train\", response_model=TrainingResponse)\n",
    "async def train_model(\n",
    "    request: TrainingRequest,\n",
    "    background_tasks: BackgroundTasks,\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \"\"\"Trigger model training\"\"\"\n",
    "    try:\n",
    "        # Queue the training job\n",
    "        background_tasks.add_task(\n",
    "            model_trainer.train_model,\n",
    "            model_type=request.model_type,\n",
    "            is_global=request.is_global,\n",
    "            user_id=request.user_id if not request.is_global else None\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"job_id\": \"training_job_placeholder\",  # Would be a real job ID in production\n",
    "            \"status\": \"queued\",\n",
    "            \"message\": f\"Training job for {request.model_type} model has been queued\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Model training error: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@api_router.get(\"/models\", response_model=List[ModelInfoResponse])\n",
    "async def get_models(\n",
    "    model_type: Optional[str] = None,\n",
    "    is_global: Optional[bool] = None,\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \"\"\"List available models\"\"\"\n",
    "    try:\n",
    "        # In a real implementation, this would query the database\n",
    "        # For now, return placeholder data\n",
    "        models = [\n",
    "            {\n",
    "                \"id\": 1,\n",
    "                \"name\": \"ARIMA Global Model\",\n",
    "                \"type\": \"arima\",\n",
    "                \"version\": \"v1.0.0\",\n",
    "                \"is_active\": True,\n",
    "                \"is_global\": True,\n",
    "                \"accuracy\": 0.87,\n",
    "                \"trained_at\": \"2023-03-15T12:00:00Z\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": 2,\n",
    "                \"name\": \"Random Forest Global Model\",\n",
    "                \"type\": \"random_forest\",\n",
    "                \"version\": \"v1.0.0\",\n",
    "                \"is_active\": True,\n",
    "                \"is_global\": True,\n",
    "                \"accuracy\": 0.85,\n",
    "                \"trained_at\": \"2023-03-15T12:00:00Z\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": 3,\n",
    "                \"name\": \"Gradient Boosting Global Model\",\n",
    "                \"type\": \"gradient_boosting\",\n",
    "                \"version\": \"v1.0.0\",\n",
    "                \"is_active\": True,\n",
    "                \"is_global\": True,\n",
    "                \"accuracy\": 0.82,\n",
    "                \"trained_at\": \"2023-03-15T12:00:00Z\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Filter by model_type if provided\n",
    "        if model_type:\n",
    "            models = [model for model in models if model[\"type\"] == model_type]\n",
    "        \n",
    "        # Filter by is_global if provided\n",
    "        if is_global is not None:\n",
    "            models = [model for model in models if model[\"is_global\"] == is_global]\n",
    "        \n",
    "        return models\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting models: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@api_router.get(\"/models/{model_id}/metrics\", response_model=ModelMetricsResponse)\n",
    "async def get_model_metrics(\n",
    "    model_id: int,\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \"\"\"Get metrics for a specific model\"\"\"\n",
    "    try:\n",
    "        # In a real implementation, this would query the database\n",
    "        # For now, return placeholder data based on model_id\n",
    "        metrics = {\n",
    "            \"mae\": 1.2,\n",
    "            \"rmse\": 1.8,\n",
    "            \"accuracy\": 0.87,\n",
    "            \"sample_size\": 1000,\n",
    "            \"feature_importance\": {\n",
    "                \"cycle_length\": 0.7,\n",
    "                \"symptoms_pms\": 0.15,\n",
    "                \"stress_level\": 0.1,\n",
    "                \"sleep_quality\": 0.05\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting model metrics: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@api_router.put(\"/models/{model_id}/activate\")\n",
    "async def activate_model(\n",
    "    model_id: int,\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \"\"\"Activate a specific model version\"\"\"\n",
    "    try:\n",
    "        # In a real implementation, this would update the database\n",
    "        # For now, return placeholder success response\n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"message\": f\"Model {model_id} activated successfully\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error activating model: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\"\"\"\n",
    "\n",
    "# 5. Create Pydantic schemas for API\n",
    "prediction_schemas_content = \"\"\"from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Optional, Any, Union\n",
    "from datetime import date, datetime\n",
    "\n",
    "# Period Prediction\n",
    "class PeriodPredictionRequest(BaseModel):\n",
    "    user_id: Optional[int] = None\n",
    "    last_period_start: Optional[date] = None\n",
    "    last_period_end: Optional[date] = None\n",
    "    average_cycle_length: Optional[float] = None\n",
    "    historical_cycles: Optional[List[Dict[str, Any]]] = None\n",
    "    current_symptoms: Optional[List[Dict[str, Any]]] = None\n",
    "\n",
    "class PeriodPredictionResponse(BaseModel):\n",
    "    next_period_date: date\n",
    "    confidence_score: float = Field(..., ge=0.0, le=1.0)\n",
    "    predicted_duration: int\n",
    "    window_start: date\n",
    "    window_end: date\n",
    "    model_version: str\n",
    "\n",
    "# Fertility Prediction\n",
    "class FertilityPredictionRequest(BaseModel):\n",
    "    user_id: Optional[int] = None\n",
    "    last_period_start: Optional[date] = None\n",
    "    average_cycle_length: Optional[float] = None\n",
    "    historical_cycles: Optional[List[Dict[str, Any]]] = None\n",
    "    basal_temperatures: Optional[List[Dict[str, Any]]] = None\n",
    "    cervical_fluid: Optional[List[Dict[str, Any]]] = None\n",
    "\n",
    "class FertilityPredictionResponse(BaseModel):\n",
    "    fertility_window_start: date\n",
    "    fertility_window_end: date\n",
    "    peak_fertility_date: date\n",
    "    confidence_score: float = Field(..., ge=0.0, le=1.0)\n",
    "    model_version: str\n",
    "\n",
    "# Ovulation Prediction\n",
    "class OvulationPredictionRequest(BaseModel):\n",
    "    user_id: Optional[int] = None\n",
    "    last_period_start: Optional[date] = None\n",
    "    average_cycle_length: Optional[float] = None\n",
    "    historical_cycles: Optional[List[Dict[str, Any]]] = None\n",
    "    basal_temperatures: Optional[List[Dict[str, Any]]] = None\n",
    "    cervical_fluid: Optional[List[Dict[str, Any]]] = None\n",
    "    ovulation_tests: Optional[List[Dict[str, Any]]] = None\n",
    "\n",
    "class OvulationPredictionResponse(BaseModel):\n",
    "    ovulation_date: date\n",
    "    confidence_score: float = Field(..., ge=0.0, le=1.0)\n",
    "    model_version: str\n",
    "\n",
    "# Prediction Feedback\n",
    "class PredictionFeedbackRequest(BaseModel):\n",
    "    prediction_id: Optional[int] = None\n",
    "    user_id: Optional[int] = None\n",
    "    prediction_type: str\n",
    "    predicted_date: date\n",
    "    actual_date: date\n",
    "    accurate: bool\n",
    "    error_days: int = Field(0, ge=0)\n",
    "    notes: Optional[str] = None\n",
    "\n",
    "class PredictionFeedbackResponse(BaseModel):\n",
    "    success: bool\n",
    "    message: str\n",
    "\"\"\"\n",
    "\n",
    "training_schemas_content = \"\"\"from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Optional, Any\n",
    "from datetime import datetime\n",
    "\n",
    "class TrainingRequest(BaseModel):\n",
    "    model_type: str = Field(..., description=\"Type of model to train (arima, random_forest, gradient_boosting, ensemble)\")\n",
    "    is_global: bool = Field(..., description=\"Whether to train global model or user-specific model\")\n",
    "    user_id: Optional[int] = Field(None, description=\"User ID for user-specific model training\")\n",
    "    force_retrain: bool = Field(False, description=\"Force retraining even if performance threshold is not met\")\n",
    "\n",
    "class TrainingResponse(BaseModel):\n",
    "    job_id: str\n",
    "    status: str\n",
    "    message: str\n",
    "\n",
    "class ModelInfoResponse(BaseModel):\n",
    "    id: int\n",
    "    name: str\n",
    "    type: str\n",
    "    version: str\n",
    "    is_active: bool\n",
    "    is_global: bool\n",
    "    accuracy: float\n",
    "    trained_at: datetime\n",
    "\n",
    "class ModelMetricsResponse(BaseModel):\n",
    "    mae: float\n",
    "    rmse: float\n",
    "    accuracy: float\n",
    "    sample_size: int\n",
    "    feature_importance: Dict[str, float]\n",
    "\"\"\"\n",
    "\n",
    "# Create __init__.py files for all modules to make them importable\n",
    "def create_init_files(root_dir):\n",
    "    \"\"\"Create __init__.py files in all subdirectories\"\"\"\n",
    "    for path in root_dir.glob('**/*'):\n",
    "        if path.is_dir() and not path.name.startswith('.'):\n",
    "            init_file = path / \"__init__.py\"\n",
    "            if not init_file.exists():\n",
    "                with open(init_file, \"w\") as f:\n",
    "                    f.write(\"# Initialize module\\n\")\n",
    "\n",
    "# Write the files to the directory structure\n",
    "os.makedirs(app_dir / \"core\", exist_ok=True)\n",
    "os.makedirs(app_dir / \"api\" / \"v1\", exist_ok=True)\n",
    "os.makedirs(app_dir / \"db\", exist_ok=True)\n",
    "os.makedirs(app_dir / \"schemas\", exist_ok=True)\n",
    "\n",
    "# Write main.py file\n",
    "with open(ml_engine_dir / \"main.py\", \"w\") as f:\n",
    "    f.write(main_content)\n",
    "\n",
    "# Write config.py file\n",
    "with open(app_dir / \"core\" / \"config.py\", \"w\") as f:\n",
    "    f.write(config_content)\n",
    "\n",
    "# Write database.py file\n",
    "with open(app_dir / \"db\" / \"database.py\", \"w\") as f:\n",
    "    f.write(db_content)\n",
    "\n",
    "# Write API router file\n",
    "with open(app_dir / \"api\" / \"v1\" / \"router.py\", \"w\") as f:\n",
    "    f.write(api_router_content)\n",
    "\n",
    "# Write schema files\n",
    "with open(app_dir / \"schemas\" / \"prediction.py\", \"w\") as f:\n",
    "    f.write(prediction_schemas_content)\n",
    "\n",
    "with open(app_dir / \"schemas\" / \"training.py\", \"w\") as f:\n",
    "    f.write(training_schemas_content)\n",
    "\n",
    "# Create all necessary __init__ files\n",
    "create_init_files(ml_engine_dir)\n",
    "\n",
    "# Create an api.py file in the api directory\n",
    "with open(app_dir / \"api\" / \"__init__.py\", \"w\") as f:\n",
    "    f.write(\"# API module\\n\")\n",
    "\n",
    "with open(app_dir / \"api\" / \"v1\" / \"__init__.py\", \"w\") as f:\n",
    "    f.write(\"# API v1 module\\n\")\n",
    "\n",
    "# Create requirements.txt file\n",
    "requirements_content = \"\"\"fastapi>=0.68.0\n",
    "uvicorn>=0.15.0\n",
    "pydantic>=1.8.2\n",
    "sqlalchemy>=1.4.23\n",
    "pandas>=1.3.3\n",
    "numpy>=1.21.2\n",
    "scikit-learn>=0.24.2\n",
    "statsmodels>=0.13.0\n",
    "mlflow>=1.20.0\n",
    "python-dotenv>=0.19.0\n",
    "psycopg2-binary>=2.9.1\n",
    "\"\"\"\n",
    "\n",
    "with open(ml_engine_dir / \"requirements.txt\", \"w\") as f:\n",
    "    f.write(requirements_content)\n",
    "\n",
    "print(f\"FastAPI application structure created successfully at {ml_engine_dir}\")\n",
    "print(\"Created the following key files:\")\n",
    "print(f\"1. {ml_engine_dir / 'main.py'} - Main FastAPI application\")\n",
    "print(f\"2. {app_dir / 'core' / 'config.py'} - Configuration settings\")\n",
    "print(f\"3. {app_dir / 'db' / 'database.py'} - Database utilities\")\n",
    "print(f\"4. {app_dir / 'api' / 'v1' / 'router.py'} - API router with endpoints\")\n",
    "print(f\"5. {app_dir / 'schemas' / 'prediction.py' and app_dir / 'schemas' / 'training.py'} - Pydantic schemas\")\n",
    "print(f\"6. {ml_engine_dir / 'requirements.txt'} - Project dependencies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f3756e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T15:58:49.089968Z",
     "iopub.status.busy": "2025-03-25T15:58:49.089532Z",
     "iopub.status.idle": "2025-03-25T15:58:49.116940Z",
     "shell.execute_reply": "2025-03-25T15:58:49.115719Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (420663405.py, line 147)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 147\u001b[0;36m\u001b[0m\n\u001b[0;31m    \"\"\"Get database session for dependency injection\"\"\"\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up directory paths\n",
    "ml_engine_dir = Path('/data/chats/90kuvd/workspace/period_tracker_ml')\n",
    "app_dir = ml_engine_dir / \"app\"\n",
    "\n",
    "# 1. Create the main FastAPI application file\n",
    "main_content = \"\"\"from fastapi import FastAPI, Depends, HTTPException, Security\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.security.api_key import APIKeyHeader, APIKey\n",
    "from starlette.status import HTTP_403_FORBIDDEN\n",
    "import logging\n",
    "import sys\n",
    "from typing import Dict, Any\n",
    "\n",
    "from app.api.v1.router import api_router\n",
    "from app.core.config import settings\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize FastAPI application\n",
    "app = FastAPI(\n",
    "    title=settings.PROJECT_NAME,\n",
    "    description=settings.PROJECT_DESCRIPTION,\n",
    "    version=settings.VERSION,\n",
    "    openapi_url=f\"{settings.API_V1_STR}/openapi.json\"\n",
    ")\n",
    "\n",
    "# API key authentication\n",
    "api_key_header = APIKeyHeader(name=settings.API_KEY_NAME, auto_error=False)\n",
    "\n",
    "async def get_api_key(api_key_header: str = Security(api_key_header)):\n",
    "    if api_key_header == settings.API_KEY:\n",
    "        return api_key_header\n",
    "    else:\n",
    "        raise HTTPException(\n",
    "            status_code=HTTP_403_FORBIDDEN, detail=\"Invalid API Key\"\n",
    "        )\n",
    "\n",
    "# Set up CORS middleware\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=settings.CORS_ORIGINS,\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Include API router\n",
    "app.include_router(\n",
    "    api_router,\n",
    "    prefix=settings.API_V1_STR,\n",
    "    dependencies=[Depends(get_api_key)] if settings.API_KEY_REQUIRED else []\n",
    ")\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    return {\"status\": \"healthy\", \"version\": settings.VERSION}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\"\"\"\n",
    "\n",
    "# 2. Create configuration file\n",
    "config_content = \"\"\"from typing import List, Optional, Union, Dict, Any\n",
    "from pydantic import BaseSettings, validator\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "class Settings(BaseSettings):\n",
    "    # API Settings\n",
    "    API_V1_STR: str = \"/api/v1\"\n",
    "    PROJECT_NAME: str = \"Period Tracker ML Engine\"\n",
    "    PROJECT_DESCRIPTION: str = \"ML Engine for the Period Tracker App\"\n",
    "    VERSION: str = \"0.1.0\"\n",
    "    \n",
    "    # Security\n",
    "    API_KEY_NAME: str = \"X-API-KEY\"\n",
    "    API_KEY: str = os.getenv(\"API_KEY\", \"development_api_key\")\n",
    "    API_KEY_REQUIRED: bool = True\n",
    "    \n",
    "    # CORS\n",
    "    CORS_ORIGINS: List[str] = [\"http://localhost:8000\", \"http://localhost:3000\"]\n",
    "    \n",
    "    # Database\n",
    "    POSTGRES_SERVER: str = os.getenv(\"POSTGRES_SERVER\", \"localhost\")\n",
    "    POSTGRES_USER: str = os.getenv(\"POSTGRES_USER\", \"postgres\")\n",
    "    POSTGRES_PASSWORD: str = os.getenv(\"POSTGRES_PASSWORD\", \"postgres\")\n",
    "    POSTGRES_DB: str = os.getenv(\"POSTGRES_DB\", \"period_tracker\")\n",
    "    SQLALCHEMY_DATABASE_URI: Optional[str] = None\n",
    "    \n",
    "    @validator(\"SQLALCHEMY_DATABASE_URI\", pre=True)\n",
    "    def assemble_db_connection(cls, v: Optional[str], values: Dict[str, Any]) -> str:\n",
    "        if v:\n",
    "            return v\n",
    "        return f\"postgresql://{values.get('POSTGRES_USER')}:{values.get('POSTGRES_PASSWORD')}@{values.get('POSTGRES_SERVER')}/{values.get('POSTGRES_DB')}\"\n",
    "    \n",
    "    # ML Settings\n",
    "    MODEL_PATH: str = os.getenv(\"MODEL_PATH\", str(Path(__file__).parent.parent.parent / \"models\"))\n",
    "    MLFLOW_TRACKING_URI: str = os.getenv(\"MLFLOW_TRACKING_URI\", \"sqlite:///mlflow.db\")\n",
    "    \n",
    "    # ML Parameters\n",
    "    MIN_CYCLES_FOR_TRAINING: int = 3\n",
    "    MIN_CYCLES_FOR_USER_MODEL: int = 6\n",
    "    RETRAINING_GLOBAL_SCHEDULE: str = \"0 0 * * 0\"  # Weekly on Sunday at midnight\n",
    "    RETRAINING_USER_SCHEDULE: str = \"0 0 1 * *\"    # Monthly on 1st at midnight\n",
    "    \n",
    "    # Performance Thresholds\n",
    "    MIN_ACCURACY_THRESHOLD: float = 0.85\n",
    "    MAX_ERROR_DAYS: int = 3\n",
    "    \n",
    "    class Config:\n",
    "        case_sensitive = True\n",
    "        env_file = \".env\"\n",
    "\n",
    "settings = Settings()\n",
    "\"\"\"\n",
    "\n",
    "# 3. Create database connection utilities\n",
    "db_content = \"\"\"from sqlalchemy import create_engine\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import pandas as pd\n",
    "from typing import Generator, Dict, Any, List\n",
    "import logging\n",
    "\n",
    "from app.core.config import settings\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Create SQLAlchemy engine and session\n",
    "engine = create_engine(settings.SQLALCHEMY_DATABASE_URI)\n",
    "SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
    "\n",
    "# Base class for database models\n",
    "Base = declarative_base()\n",
    "\n",
    "def get_db() -> Generator:\n",
    "    \"\"\"Get database session for dependency injection\"\"\"\n",
    "    db = SessionLocal()\n",
    "    try:\n",
    "        yield db\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "async def fetch_user_data(user_id: int) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fetch user data from the database for predictions\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create an SQL query to get user cycle data, symptoms, etc.\n",
    "        query = f'''\n",
    "        SELECT \n",
    "            c.id, \n",
    "            c.user_id,\n",
    "            c.start_date, \n",
    "            c.end_date, \n",
    "            c.duration, \n",
    "            c.notes\n",
    "        FROM \n",
    "            cycles c\n",
    "        WHERE \n",
    "            c.user_id = {user_id}\n",
    "        ORDER BY \n",
    "            c.start_date DESC\n",
    "        '''\n",
    "        \n",
    "        # Execute query and convert to pandas DataFrame\n",
    "        with engine.connect() as connection:\n",
    "            cycles_df = pd.read_sql(query, connection)\n",
    "        \n",
    "        # Query for user's symptoms\n",
    "        symptom_query = f'''\n",
    "        SELECT \n",
    "            sl.id,\n",
    "            sl.user_id,\n",
    "            sl.symptom_id,\n",
    "            sl.log_date,\n",
    "            sl.severity,\n",
    "            sl.notes,\n",
    "            s.name as symptom_name,\n",
    "            s.category\n",
    "        FROM \n",
    "            symptom_logs sl\n",
    "        JOIN\n",
    "            symptoms s ON sl.symptom_id = s.id\n",
    "        WHERE \n",
    "            sl.user_id = {user_id}\n",
    "        ORDER BY \n",
    "            sl.log_date DESC\n",
    "        '''\n",
    "        \n",
    "        with engine.connect() as connection:\n",
    "            symptoms_df = pd.read_sql(symptom_query, connection)\n",
    "        \n",
    "        # Query for biometric data\n",
    "        biometric_query = f'''\n",
    "        SELECT \n",
    "            id,\n",
    "            user_id,\n",
    "            type,\n",
    "            value,\n",
    "            log_date,\n",
    "            notes\n",
    "        FROM \n",
    "            biometric_logs\n",
    "        WHERE \n",
    "            user_id = {user_id}\n",
    "        ORDER BY \n",
    "            log_date DESC\n",
    "        '''\n",
    "        \n",
    "        with engine.connect() as connection:\n",
    "            biometrics_df = pd.read_sql(biometric_query, connection)\n",
    "        \n",
    "        # Process the data into the required format\n",
    "        user_data = {\n",
    "            \"user_id\": user_id,\n",
    "            \"historical_cycles\": cycles_df.to_dict('records'),\n",
    "            \"symptoms\": symptoms_df.to_dict('records'),\n",
    "            \"biometrics\": biometrics_df.to_dict('records'),\n",
    "        }\n",
    "        \n",
    "        # Calculate average cycle length if there's data\n",
    "        if not cycles_df.empty:\n",
    "            user_data[\"average_cycle_length\"] = cycles_df[\"duration\"].mean()\n",
    "            user_data[\"last_period_start\"] = cycles_df.iloc[0][\"start_date\"]\n",
    "        \n",
    "        return user_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching user data: {str(e)}\")\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "async def fetch_training_data(is_anonymous: bool = True) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Fetch data for model training from the database\n",
    "    \n",
    "    Args:\n",
    "        is_anonymous: If True, anonymizes the data\n",
    "        \n",
    "    Returns:\n",
    "        Dict with DataFrames for cycles, symptoms, biometrics, etc.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create SQL queries for the required data\n",
    "        cycles_query = '''\n",
    "        SELECT \n",
    "            c.id, \n",
    "            c.user_id,\n",
    "            c.start_date, \n",
    "            c.end_date, \n",
    "            c.duration\n",
    "        FROM \n",
    "            cycles c\n",
    "        ORDER BY \n",
    "            c.user_id, c.start_date\n",
    "        '''\n",
    "        \n",
    "        symptoms_query = '''\n",
    "        SELECT \n",
    "            sl.id,\n",
    "            sl.user_id,\n",
    "            sl.symptom_id,\n",
    "            sl.log_date,\n",
    "            sl.severity,\n",
    "            s.name as symptom_name,\n",
    "            s.category\n",
    "        FROM \n",
    "            symptom_logs sl\n",
    "        JOIN\n",
    "            symptoms s ON sl.symptom_id = s.id\n",
    "        ORDER BY \n",
    "            sl.user_id, sl.log_date\n",
    "        '''\n",
    "        \n",
    "        biometrics_query = '''\n",
    "        SELECT \n",
    "            id,\n",
    "            user_id,\n",
    "            type,\n",
    "            value,\n",
    "            log_date\n",
    "        FROM \n",
    "            biometric_logs\n",
    "        ORDER BY \n",
    "            user_id, log_date\n",
    "        '''\n",
    "        \n",
    "        # Execute queries and convert to pandas DataFrames\n",
    "        with engine.connect() as connection:\n",
    "            cycles_df = pd.read_sql(cycles_query, connection)\n",
    "            symptoms_df = pd.read_sql(symptoms_query, connection)\n",
    "            biometrics_df = pd.read_sql(biometrics_query, connection)\n",
    "        \n",
    "        # Anonymize data if requested\n",
    "        if is_anonymous:\n",
    "            # Replace user_id with anonymous identifiers\n",
    "            user_mapping = {}\n",
    "            for df in [cycles_df, symptoms_df, biometrics_df]:\n",
    "                if 'user_id' in df.columns:\n",
    "                    for user_id in df['user_id'].unique():\n",
    "                        if user_id not in user_mapping:\n",
    "                            user_mapping[user_id] = f\"anon_{len(user_mapping)+1}\"\n",
    "                    df['user_id'] = df['user_id'].map(user_mapping)\n",
    "        \n",
    "        return {\n",
    "            \"cycles\": cycles_df,\n",
    "            \"symptoms\": symptoms_df,\n",
    "            \"biometrics\": biometrics_df\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching training data: {str(e)}\")\n",
    "        return {\"error\": str(e)}\n",
    "\"\"\"\n",
    "\n",
    "# 4. Create API router for v1\n",
    "api_router_content = \"\"\"from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks\n",
    "from sqlalchemy.orm import Session\n",
    "from typing import List, Dict, Any, Optional\n",
    "import logging\n",
    "\n",
    "from app.db.database import get_db\n",
    "from app.schemas.prediction import (\n",
    "    PeriodPredictionRequest, \n",
    "    PeriodPredictionResponse, \n",
    "    FertilityPredictionRequest, \n",
    "    FertilityPredictionResponse,\n",
    "    OvulationPredictionRequest,\n",
    "    OvulationPredictionResponse,\n",
    "    PredictionFeedbackRequest,\n",
    "    PredictionFeedbackResponse\n",
    ")\n",
    "from app.schemas.training import (\n",
    "    TrainingRequest, \n",
    "    TrainingResponse, \n",
    "    ModelInfoResponse,\n",
    "    ModelMetricsResponse\n",
    ")\n",
    "from app.models.ensemble.prediction_service import PredictionService\n",
    "from app.training.model_trainer import ModelTrainer\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "api_router = APIRouter()\n",
    "prediction_service = PredictionService()\n",
    "model_trainer = ModelTrainer()\n",
    "\n",
    "# Prediction endpoints\n",
    "@api_router.post(\"/predictions/period\", response_model=PeriodPredictionResponse)\n",
    "async def predict_period(\n",
    "    request: PeriodPredictionRequest,\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \"\"\"Predict next period date\"\"\"\n",
    "    try:\n",
    "        # Prepare input data for prediction\n",
    "        prediction_data = request.dict()\n",
    "        \n",
    "        # If user_id is provided, fetch additional data from database\n",
    "        if request.user_id:\n",
    "            from app.db.database import fetch_user_data\n",
    "            user_data = await fetch_user_data(request.user_id)\n",
    "            prediction_data.update(user_data)\n",
    "        \n",
    "        # Generate prediction\n",
    "        prediction_result = prediction_service.predict_period(prediction_data)\n",
    "        \n",
    "        return prediction_result\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Period prediction error: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@api_router.post(\"/predictions/fertility\", response_model=FertilityPredictionResponse)\n",
    "async def predict_fertility(\n",
    "    request: FertilityPredictionRequest,\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \"\"\"Predict fertility window\"\"\"\n",
    "    try:\n",
    "        # Prepare input data for prediction\n",
    "        prediction_data = request.dict()\n",
    "        \n",
    "        # If user_id is provided, fetch additional data from database\n",
    "        if request.user_id:\n",
    "            from app.db.database import fetch_user_data\n",
    "            user_data = await fetch_user_data(request.user_id)\n",
    "            prediction_data.update(user_data)\n",
    "        \n",
    "        # Generate prediction\n",
    "        prediction_result = prediction_service.predict_fertility(prediction_data)\n",
    "        \n",
    "        return prediction_result\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fertility prediction error: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@api_router.post(\"/predictions/ovulation\", response_model=OvulationPredictionResponse)\n",
    "async def predict_ovulation(\n",
    "    request: OvulationPredictionRequest,\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \"\"\"Predict ovulation day\"\"\"\n",
    "    try:\n",
    "        # Prepare input data for prediction\n",
    "        prediction_data = request.dict()\n",
    "        \n",
    "        # If user_id is provided, fetch additional data from database\n",
    "        if request.user_id:\n",
    "            from app.db.database import fetch_user_data\n",
    "            user_data = await fetch_user_data(request.user_id)\n",
    "            prediction_data.update(user_data)\n",
    "        \n",
    "        # Generate prediction\n",
    "        prediction_result = prediction_service.predict_ovulation(prediction_data)\n",
    "        \n",
    "        return prediction_result\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Ovulation prediction error: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@api_router.post(\"/predictions/feedback\", response_model=PredictionFeedbackResponse)\n",
    "async def submit_feedback(\n",
    "    request: PredictionFeedbackRequest,\n",
    "    background_tasks: BackgroundTasks,\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \"\"\"Submit feedback on a prediction\"\"\"\n",
    "    try:\n",
    "        # Store the feedback in the database\n",
    "        # This would be implemented in a real system\n",
    "        \n",
    "        # Queue model evaluation if sufficient feedback is collected\n",
    "        background_tasks.add_task(\n",
    "            model_trainer.evaluate_model_performance, \n",
    "            user_id=request.user_id if request.user_id else None\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"message\": \"Feedback submitted successfully\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Feedback submission error: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# Model training and management endpoints\n",
    "@api_router.post(\"/models/train\", response_model=TrainingResponse)\n",
    "async def train_model(\n",
    "    request: TrainingRequest,\n",
    "    background_tasks: BackgroundTasks,\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \"\"\"Trigger model training\"\"\"\n",
    "    try:\n",
    "        # Queue the training job\n",
    "        background_tasks.add_task(\n",
    "            model_trainer.train_model,\n",
    "            model_type=request.model_type,\n",
    "            is_global=request.is_global,\n",
    "            user_id=request.user_id if not request.is_global else None\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"job_id\": \"training_job_placeholder\",  # Would be a real job ID in production\n",
    "            \"status\": \"queued\",\n",
    "            \"message\": f\"Training job for {request.model_type} model has been queued\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Model training error: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@api_router.get(\"/models\", response_model=List[ModelInfoResponse])\n",
    "async def get_models(\n",
    "    model_type: Optional[str] = None,\n",
    "    is_global: Optional[bool] = None,\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \"\"\"List available models\"\"\"\n",
    "    try:\n",
    "        # In a real implementation, this would query the database\n",
    "        # For now, return placeholder data\n",
    "        models = [\n",
    "            {\n",
    "                \"id\": 1,\n",
    "                \"name\": \"ARIMA Global Model\",\n",
    "                \"type\": \"arima\",\n",
    "                \"version\": \"v1.0.0\",\n",
    "                \"is_active\": True,\n",
    "                \"is_global\": True,\n",
    "                \"accuracy\": 0.87,\n",
    "                \"trained_at\": \"2023-03-15T12:00:00Z\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": 2,\n",
    "                \"name\": \"Random Forest Global Model\",\n",
    "                \"type\": \"random_forest\",\n",
    "                \"version\": \"v1.0.0\",\n",
    "                \"is_active\": True,\n",
    "                \"is_global\": True,\n",
    "                \"accuracy\": 0.85,\n",
    "                \"trained_at\": \"2023-03-15T12:00:00Z\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": 3,\n",
    "                \"name\": \"Gradient Boosting Global Model\",\n",
    "                \"type\": \"gradient_boosting\",\n",
    "                \"version\": \"v1.0.0\",\n",
    "                \"is_active\": True,\n",
    "                \"is_global\": True,\n",
    "                \"accuracy\": 0.82,\n",
    "                \"trained_at\": \"2023-03-15T12:00:00Z\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Filter by model_type if provided\n",
    "        if model_type:\n",
    "            models = [model for model in models if model[\"type\"] == model_type]\n",
    "        \n",
    "        # Filter by is_global if provided\n",
    "        if is_global is not None:\n",
    "            models = [model for model in models if model[\"is_global\"] == is_global]\n",
    "        \n",
    "        return models\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting models: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@api_router.get(\"/models/{model_id}/metrics\", response_model=ModelMetricsResponse)\n",
    "async def get_model_metrics(\n",
    "    model_id: int,\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \"\"\"Get metrics for a specific model\"\"\"\n",
    "    try:\n",
    "        # In a real implementation, this would query the database\n",
    "        # For now, return placeholder data based on model_id\n",
    "        metrics = {\n",
    "            \"mae\": 1.2,\n",
    "            \"rmse\": 1.8,\n",
    "            \"accuracy\": 0.87,\n",
    "            \"sample_size\": 1000,\n",
    "            \"feature_importance\": {\n",
    "                \"cycle_length\": 0.7,\n",
    "                \"symptoms_pms\": 0.15,\n",
    "                \"stress_level\": 0.1,\n",
    "                \"sleep_quality\": 0.05\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting model metrics: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@api_router.put(\"/models/{model_id}/activate\")\n",
    "async def activate_model(\n",
    "    model_id: int,\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \"\"\"Activate a specific model version\"\"\"\n",
    "    try:\n",
    "        # In a real implementation, this would update the database\n",
    "        # For now, return placeholder success response\n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"message\": f\"Model {model_id} activated successfully\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error activating model: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\"\"\"\n",
    "\n",
    "# 5. Create Pydantic schemas for API\n",
    "prediction_schemas_content = \"\"\"from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Optional, Any, Union\n",
    "from datetime import date, datetime\n",
    "\n",
    "# Period Prediction\n",
    "class PeriodPredictionRequest(BaseModel):\n",
    "    user_id: Optional[int] = None\n",
    "    last_period_start: Optional[date] = None\n",
    "    last_period_end: Optional[date] = None\n",
    "    average_cycle_length: Optional[float] = None\n",
    "    historical_cycles: Optional[List[Dict[str, Any]]] = None\n",
    "    current_symptoms: Optional[List[Dict[str, Any]]] = None\n",
    "\n",
    "class PeriodPredictionResponse(BaseModel):\n",
    "    next_period_date: date\n",
    "    confidence_score: float = Field(..., ge=0.0, le=1.0)\n",
    "    predicted_duration: int\n",
    "    window_start: date\n",
    "    window_end: date\n",
    "    model_version: str\n",
    "\n",
    "# Fertility Prediction\n",
    "class FertilityPredictionRequest(BaseModel):\n",
    "    user_id: Optional[int] = None\n",
    "    last_period_start: Optional[date] = None\n",
    "    average_cycle_length: Optional[float] = None\n",
    "    historical_cycles: Optional[List[Dict[str, Any]]] = None\n",
    "    basal_temperatures: Optional[List[Dict[str, Any]]] = None\n",
    "    cervical_fluid: Optional[List[Dict[str, Any]]] = None\n",
    "\n",
    "class FertilityPredictionResponse(BaseModel):\n",
    "    fertility_window_start: date\n",
    "    fertility_window_end: date\n",
    "    peak_fertility_date: date\n",
    "    confidence_score: float = Field(..., ge=0.0, le=1.0)\n",
    "    model_version: str\n",
    "\n",
    "# Ovulation Prediction\n",
    "class OvulationPredictionRequest(BaseModel):\n",
    "    user_id: Optional[int] = None\n",
    "    last_period_start: Optional[date] = None\n",
    "    average_cycle_length: Optional[float] = None\n",
    "    historical_cycles: Optional[List[Dict[str, Any]]] = None\n",
    "    basal_temperatures: Optional[List[Dict[str, Any]]] = None\n",
    "    cervical_fluid: Optional[List[Dict[str, Any]]] = None\n",
    "    ovulation_tests: Optional[List[Dict[str, Any]]] = None\n",
    "\n",
    "class OvulationPredictionResponse(BaseModel):\n",
    "    ovulation_date: date\n",
    "    confidence_score: float = Field(..., ge=0.0, le=1.0)\n",
    "    model_version: str\n",
    "\n",
    "# Prediction Feedback\n",
    "class PredictionFeedbackRequest(BaseModel):\n",
    "    prediction_id: Optional[int] = None\n",
    "    user_id: Optional[int] = None\n",
    "    prediction_type: str\n",
    "    predicted_date: date\n",
    "    actual_date: date\n",
    "    accurate: bool\n",
    "    error_days: int = Field(0, ge=0)\n",
    "    notes: Optional[str] = None\n",
    "\n",
    "class PredictionFeedbackResponse(BaseModel):\n",
    "    success: bool\n",
    "    message: str\n",
    "\"\"\"\n",
    "\n",
    "training_schemas_content = \"\"\"from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Optional, Any\n",
    "from datetime import datetime\n",
    "\n",
    "class TrainingRequest(BaseModel):\n",
    "    model_type: str = Field(..., description=\"Type of model to train (arima, random_forest, gradient_boosting, ensemble)\")\n",
    "    is_global: bool = Field(..., description=\"Whether to train global model or user-specific model\")\n",
    "    user_id: Optional[int] = Field(None, description=\"User ID for user-specific model training\")\n",
    "    force_retrain: bool = Field(False, description=\"Force retraining even if performance threshold is not met\")\n",
    "\n",
    "class TrainingResponse(BaseModel):\n",
    "    job_id: str\n",
    "    status: str\n",
    "    message: str\n",
    "\n",
    "class ModelInfoResponse(BaseModel):\n",
    "    id: int\n",
    "    name: str\n",
    "    type: str\n",
    "    version: str\n",
    "    is_active: bool\n",
    "    is_global: bool\n",
    "    accuracy: float\n",
    "    trained_at: datetime\n",
    "\n",
    "class ModelMetricsResponse(BaseModel):\n",
    "    mae: float\n",
    "    rmse: float\n",
    "    accuracy: float\n",
    "    sample_size: int\n",
    "    feature_importance: Dict[str, float]\n",
    "\"\"\"\n",
    "\n",
    "# Create __init__.py files for all modules to make them importable\n",
    "def create_init_files(root_dir):\n",
    "    \"\"\"Create __init__.py files in all subdirectories\"\"\"\n",
    "    for path in root_dir.glob('**/*'):\n",
    "        if path.is_dir() and not path.name.startswith('.'):\n",
    "            init_file = path / \"__init__.py\"\n",
    "            if not init_file.exists():\n",
    "                with open(init_file, \"w\") as f:\n",
    "                    f.write(\"# Initialize module\\n\")\n",
    "\n",
    "# Write the files to the directory structure\n",
    "os.makedirs(app_dir / \"core\", exist_ok=True)\n",
    "os.makedirs(app_dir / \"api\" / \"v1\", exist_ok=True)\n",
    "os.makedirs(app_dir / \"db\", exist_ok=True)\n",
    "os.makedirs(app_dir / \"schemas\", exist_ok=True)\n",
    "\n",
    "# Write main.py file\n",
    "with open(ml_engine_dir / \"main.py\", \"w\") as f:\n",
    "    f.write(main_content)\n",
    "\n",
    "# Write config.py file\n",
    "with open(app_dir / \"core\" / \"config.py\", \"w\") as f:\n",
    "    f.write(config_content)\n",
    "\n",
    "# Write database.py file\n",
    "with open(app_dir / \"db\" / \"database.py\", \"w\") as f:\n",
    "    f.write(db_content)\n",
    "\n",
    "# Write API router file\n",
    "with open(app_dir / \"api\" / \"v1\" / \"router.py\", \"w\") as f:\n",
    "    f.write(api_router_content)\n",
    "\n",
    "# Write schema files\n",
    "with open(app_dir / \"schemas\" / \"prediction.py\", \"w\") as f:\n",
    "    f.write(prediction_schemas_content)\n",
    "\n",
    "with open(app_dir / \"schemas\" / \"training.py\", \"w\") as f:\n",
    "    f.write(training_schemas_content)\n",
    "\n",
    "# Create all necessary __init__ files\n",
    "create_init_files(ml_engine_dir)\n",
    "\n",
    "# Create an api.py file in the api directory\n",
    "with open(app_dir / \"api\" / \"__init__.py\", \"w\") as f:\n",
    "    f.write(\"# API module\\n\")\n",
    "\n",
    "with open(app_dir / \"api\" / \"v1\" / \"__init__.py\", \"w\") as f:\n",
    "    f.write(\"# API v1 module\\n\")\n",
    "\n",
    "# Create requirements.txt file\n",
    "requirements_content = \"\"\"fastapi>=0.68.0\n",
    "uvicorn>=0.15.0\n",
    "pydantic>=1.8.2\n",
    "sqlalchemy>=1.4.23\n",
    "pandas>=1.3.3\n",
    "numpy>=1.21.2\n",
    "scikit-learn>=0.24.2\n",
    "statsmodels>=0.13.0\n",
    "mlflow>=1.20.0\n",
    "python-dotenv>=0.19.0\n",
    "psycopg2-binary>=2.9.1\n",
    "\"\"\"\n",
    "\n",
    "with open(ml_engine_dir / \"requirements.txt\", \"w\") as f:\n",
    "    f.write(requirements_content)\n",
    "\n",
    "print(f\"FastAPI application structure created successfully at {ml_engine_dir}\")\n",
    "print(\"Created the following key files:\")\n",
    "print(f\"1. {ml_engine_dir / 'main.py'} - Main FastAPI application\")\n",
    "print(f\"2. {app_dir / 'core' / 'config.py'} - Configuration settings\")\n",
    "print(f\"3. {app_dir / 'db' / 'database.py'} - Database utilities\")\n",
    "print(f\"4. {app_dir / 'api' / 'v1' / 'router.py'} - API router with endpoints\")\n",
    "print(f\"5. {app_dir / 'schemas' / 'prediction.py' and app_dir / 'schemas' / 'training.py'} - Pydantic schemas\")\n",
    "print(f\"6. {ml_engine_dir / 'requirements.txt'} - Project dependencies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9245c495",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T16:00:33.923364Z",
     "iopub.status.busy": "2025-03-25T16:00:33.923098Z",
     "iopub.status.idle": "2025-03-25T16:00:33.943025Z",
     "shell.execute_reply": "2025-03-25T16:00:33.942202Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1802076002.py, line 147)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 147\u001b[0;36m\u001b[0m\n\u001b[0;31m    \"\"\"Get database session for dependency injection.\"\"\"\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up directory paths\n",
    "ml_engine_dir = Path('/data/chats/90kuvd/workspace/period_tracker_ml')\n",
    "app_dir = ml_engine_dir / \"app\"\n",
    "\n",
    "# 1. Create the main FastAPI application file\n",
    "main_content = \"\"\"from fastapi import FastAPI, Depends, HTTPException, Security\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.security.api_key import APIKeyHeader, APIKey\n",
    "from starlette.status import HTTP_403_FORBIDDEN\n",
    "import logging\n",
    "import sys\n",
    "from typing import Dict, Any\n",
    "\n",
    "from app.api.v1.router import api_router\n",
    "from app.core.config import settings\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize FastAPI application\n",
    "app = FastAPI(\n",
    "    title=settings.PROJECT_NAME,\n",
    "    description=settings.PROJECT_DESCRIPTION,\n",
    "    version=settings.VERSION,\n",
    "    openapi_url=f\"{settings.API_V1_STR}/openapi.json\"\n",
    ")\n",
    "\n",
    "# API key authentication\n",
    "api_key_header = APIKeyHeader(name=settings.API_KEY_NAME, auto_error=False)\n",
    "\n",
    "async def get_api_key(api_key_header: str = Security(api_key_header)):\n",
    "    if api_key_header == settings.API_KEY:\n",
    "        return api_key_header\n",
    "    else:\n",
    "        raise HTTPException(\n",
    "            status_code=HTTP_403_FORBIDDEN, detail=\"Invalid API Key\"\n",
    "        )\n",
    "\n",
    "# Set up CORS middleware\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=settings.CORS_ORIGINS,\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Include API router\n",
    "app.include_router(\n",
    "    api_router,\n",
    "    prefix=settings.API_V1_STR,\n",
    "    dependencies=[Depends(get_api_key)] if settings.API_KEY_REQUIRED else []\n",
    ")\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    return {\"status\": \"healthy\", \"version\": settings.VERSION}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\"\"\"\n",
    "\n",
    "# 2. Create configuration file\n",
    "config_content = \"\"\"from typing import List, Optional, Union, Dict, Any\n",
    "from pydantic import BaseSettings, validator\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "class Settings(BaseSettings):\n",
    "    # API Settings\n",
    "    API_V1_STR: str = \"/api/v1\"\n",
    "    PROJECT_NAME: str = \"Period Tracker ML Engine\"\n",
    "    PROJECT_DESCRIPTION: str = \"ML Engine for the Period Tracker App\"\n",
    "    VERSION: str = \"0.1.0\"\n",
    "    \n",
    "    # Security\n",
    "    API_KEY_NAME: str = \"X-API-KEY\"\n",
    "    API_KEY: str = os.getenv(\"API_KEY\", \"development_api_key\")\n",
    "    API_KEY_REQUIRED: bool = True\n",
    "    \n",
    "    # CORS\n",
    "    CORS_ORIGINS: List[str] = [\"http://localhost:8000\", \"http://localhost:3000\"]\n",
    "    \n",
    "    # Database\n",
    "    POSTGRES_SERVER: str = os.getenv(\"POSTGRES_SERVER\", \"localhost\")\n",
    "    POSTGRES_USER: str = os.getenv(\"POSTGRES_USER\", \"postgres\")\n",
    "    POSTGRES_PASSWORD: str = os.getenv(\"POSTGRES_PASSWORD\", \"postgres\")\n",
    "    POSTGRES_DB: str = os.getenv(\"POSTGRES_DB\", \"period_tracker\")\n",
    "    SQLALCHEMY_DATABASE_URI: Optional[str] = None\n",
    "    \n",
    "    @validator(\"SQLALCHEMY_DATABASE_URI\", pre=True)\n",
    "    def assemble_db_connection(cls, v: Optional[str], values: Dict[str, Any]) -> str:\n",
    "        if v:\n",
    "            return v\n",
    "        return f\"postgresql://{values.get('POSTGRES_USER')}:{values.get('POSTGRES_PASSWORD')}@{values.get('POSTGRES_SERVER')}/{values.get('POSTGRES_DB')}\"\n",
    "    \n",
    "    # ML Settings\n",
    "    MODEL_PATH: str = os.getenv(\"MODEL_PATH\", str(Path(__file__).parent.parent.parent / \"models\"))\n",
    "    MLFLOW_TRACKING_URI: str = os.getenv(\"MLFLOW_TRACKING_URI\", \"sqlite:///mlflow.db\")\n",
    "    \n",
    "    # ML Parameters\n",
    "    MIN_CYCLES_FOR_TRAINING: int = 3\n",
    "    MIN_CYCLES_FOR_USER_MODEL: int = 6\n",
    "    RETRAINING_GLOBAL_SCHEDULE: str = \"0 0 * * 0\"  # Weekly on Sunday at midnight\n",
    "    RETRAINING_USER_SCHEDULE: str = \"0 0 1 * *\"    # Monthly on 1st at midnight\n",
    "    \n",
    "    # Performance Thresholds\n",
    "    MIN_ACCURACY_THRESHOLD: float = 0.85\n",
    "    MAX_ERROR_DAYS: int = 3\n",
    "    \n",
    "    class Config:\n",
    "        case_sensitive = True\n",
    "        env_file = \".env\"\n",
    "\n",
    "settings = Settings()\n",
    "\"\"\"\n",
    "\n",
    "# 3. Create database connection utilities - Fixed the docstring syntax error\n",
    "db_content = \"\"\"from sqlalchemy import create_engine\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import pandas as pd\n",
    "from typing import Generator, Dict, Any, List\n",
    "import logging\n",
    "\n",
    "from app.core.config import settings\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Create SQLAlchemy engine and session\n",
    "engine = create_engine(settings.SQLALCHEMY_DATABASE_URI)\n",
    "SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
    "\n",
    "# Base class for database models\n",
    "Base = declarative_base()\n",
    "\n",
    "def get_db() -> Generator:\n",
    "    \"\"\"Get database session for dependency injection.\"\"\"\n",
    "    db = SessionLocal()\n",
    "    try:\n",
    "        yield db\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "async def fetch_user_data(user_id: int) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fetch user data from the database for predictions\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create an SQL query to get user cycle data, symptoms, etc.\n",
    "        query = f'''\n",
    "        SELECT \n",
    "            c.id, \n",
    "            c.user_id,\n",
    "            c.start_date, \n",
    "            c.end_date, \n",
    "            c.duration, \n",
    "            c.notes\n",
    "        FROM \n",
    "            cycles c\n",
    "        WHERE \n",
    "            c.user_id = {user_id}\n",
    "        ORDER BY \n",
    "            c.start_date DESC\n",
    "        '''\n",
    "        \n",
    "        # Execute query and convert to pandas DataFrame\n",
    "        with engine.connect() as connection:\n",
    "            cycles_df = pd.read_sql(query, connection)\n",
    "        \n",
    "        # Query for user's symptoms\n",
    "        symptom_query = f'''\n",
    "        SELECT \n",
    "            sl.id,\n",
    "            sl.user_id,\n",
    "            sl.symptom_id,\n",
    "            sl.log_date,\n",
    "            sl.severity,\n",
    "            sl.notes,\n",
    "            s.name as symptom_name,\n",
    "            s.category\n",
    "        FROM \n",
    "            symptom_logs sl\n",
    "        JOIN\n",
    "            symptoms s ON sl.symptom_id = s.id\n",
    "        WHERE \n",
    "            sl.user_id = {user_id}\n",
    "        ORDER BY \n",
    "            sl.log_date DESC\n",
    "        '''\n",
    "        \n",
    "        with engine.connect() as connection:\n",
    "            symptoms_df = pd.read_sql(symptom_query, connection)\n",
    "        \n",
    "        # Query for biometric data\n",
    "        biometric_query = f'''\n",
    "        SELECT \n",
    "            id,\n",
    "            user_id,\n",
    "            type,\n",
    "            value,\n",
    "            log_date,\n",
    "            notes\n",
    "        FROM \n",
    "            biometric_logs\n",
    "        WHERE \n",
    "            user_id = {user_id}\n",
    "        ORDER BY \n",
    "            log_date DESC\n",
    "        '''\n",
    "        \n",
    "        with engine.connect() as connection:\n",
    "            biometrics_df = pd.read_sql(biometric_query, connection)\n",
    "        \n",
    "        # Process the data into the required format\n",
    "        user_data = {\n",
    "            \"user_id\": user_id,\n",
    "            \"historical_cycles\": cycles_df.to_dict('records'),\n",
    "            \"symptoms\": symptoms_df.to_dict('records'),\n",
    "            \"biometrics\": biometrics_df.to_dict('records'),\n",
    "        }\n",
    "        \n",
    "        # Calculate average cycle length if there's data\n",
    "        if not cycles_df.empty:\n",
    "            user_data[\"average_cycle_length\"] = cycles_df[\"duration\"].mean()\n",
    "            user_data[\"last_period_start\"] = cycles_df.iloc[0][\"start_date\"]\n",
    "        \n",
    "        return user_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching user data: {str(e)}\")\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "async def fetch_training_data(is_anonymous: bool = True) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Fetch data for model training from the database\n",
    "    \n",
    "    Args:\n",
    "        is_anonymous: If True, anonymizes the data\n",
    "        \n",
    "    Returns:\n",
    "        Dict with DataFrames for cycles, symptoms, biometrics, etc.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create SQL queries for the required data\n",
    "        cycles_query = '''\n",
    "        SELECT \n",
    "            c.id, \n",
    "            c.user_id,\n",
    "            c.start_date, \n",
    "            c.end_date, \n",
    "            c.duration\n",
    "        FROM \n",
    "            cycles c\n",
    "        ORDER BY \n",
    "            c.user_id, c.start_date\n",
    "        '''\n",
    "        \n",
    "        symptoms_query = '''\n",
    "        SELECT \n",
    "            sl.id,\n",
    "            sl.user_id,\n",
    "            sl.symptom_id,\n",
    "            sl.log_date,\n",
    "            sl.severity,\n",
    "            s.name as symptom_name,\n",
    "            s.category\n",
    "        FROM \n",
    "            symptom_logs sl\n",
    "        JOIN\n",
    "            symptoms s ON sl.symptom_id = s.id\n",
    "        ORDER BY \n",
    "            sl.user_id, sl.log_date\n",
    "        '''\n",
    "        \n",
    "        biometrics_query = '''\n",
    "        SELECT \n",
    "            id,\n",
    "            user_id,\n",
    "            type,\n",
    "            value,\n",
    "            log_date\n",
    "        FROM \n",
    "            biometric_logs\n",
    "        ORDER BY \n",
    "            user_id, log_date\n",
    "        '''\n",
    "        \n",
    "        # Execute queries and convert to pandas DataFrames\n",
    "        with engine.connect() as connection:\n",
    "            cycles_df = pd.read_sql(cycles_query, connection)\n",
    "            symptoms_df = pd.read_sql(symptoms_query, connection)\n",
    "            biometrics_df = pd.read_sql(biometrics_query, connection)\n",
    "        \n",
    "        # Anonymize data if requested\n",
    "        if is_anonymous:\n",
    "            # Replace user_id with anonymous identifiers\n",
    "            user_mapping = {}\n",
    "            for df in [cycles_df, symptoms_df, biometrics_df]:\n",
    "                if 'user_id' in df.columns:\n",
    "                    for user_id in df['user_id'].unique():\n",
    "                        if user_id not in user_mapping:\n",
    "                            user_mapping[user_id] = f\"anon_{len(user_mapping)+1}\"\n",
    "                    df['user_id'] = df['user_id'].map(user_mapping)\n",
    "        \n",
    "        return {\n",
    "            \"cycles\": cycles_df,\n",
    "            \"symptoms\": symptoms_df,\n",
    "            \"biometrics\": biometrics_df\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching training data: {str(e)}\")\n",
    "        return {\"error\": str(e)}\n",
    "\"\"\"\n",
    "\n",
    "# 4. Create API router for v1\n",
    "api_router_content = \"\"\"from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks\n",
    "from sqlalchemy.orm import Session\n",
    "from typing import List, Dict, Any, Optional\n",
    "import logging\n",
    "\n",
    "from app.db.database import get_db\n",
    "from app.schemas.prediction import (\n",
    "    PeriodPredictionRequest, \n",
    "    PeriodPredictionResponse, \n",
    "    FertilityPredictionRequest, \n",
    "    FertilityPredictionResponse,\n",
    "    OvulationPredictionRequest,\n",
    "    OvulationPredictionResponse,\n",
    "    PredictionFeedbackRequest,\n",
    "    PredictionFeedbackResponse\n",
    ")\n",
    "from app.schemas.training import (\n",
    "    TrainingRequest, \n",
    "    TrainingResponse, \n",
    "    ModelInfoResponse,\n",
    "    ModelMetricsResponse\n",
    ")\n",
    "from app.models.ensemble.prediction_service import PredictionService\n",
    "from app.training.model_trainer import ModelTrainer\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "api_router = APIRouter()\n",
    "prediction_service = PredictionService()\n",
    "model_trainer = ModelTrainer()\n",
    "\n",
    "# Prediction endpoints\n",
    "@api_router.post(\"/predictions/period\", response_model=PeriodPredictionResponse)\n",
    "async def predict_period(\n",
    "    request: PeriodPredictionRequest,\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \"\"\"Predict next period date\"\"\"\n",
    "    try:\n",
    "        # Prepare input data for prediction\n",
    "        prediction_data = request.dict()\n",
    "        \n",
    "        # If user_id is provided, fetch additional data from database\n",
    "        if request.user_id:\n",
    "            from app.db.database import fetch_user_data\n",
    "            user_data = await fetch_user_data(request.user_id)\n",
    "            prediction_data.update(user_data)\n",
    "        \n",
    "        # Generate prediction\n",
    "        prediction_result = prediction_service.predict_period(prediction_data)\n",
    "        \n",
    "        return prediction_result\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Period prediction error: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@api_router.post(\"/predictions/fertility\", response_model=FertilityPredictionResponse)\n",
    "async def predict_fertility(\n",
    "    request: FertilityPredictionRequest,\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \"\"\"Predict fertility window\"\"\"\n",
    "    try:\n",
    "        # Prepare input data for prediction\n",
    "        prediction_data = request.dict()\n",
    "        \n",
    "        # If user_id is provided, fetch additional data from database\n",
    "        if request.user_id:\n",
    "            from app.db.database import fetch_user_data\n",
    "            user_data = await fetch_user_data(request.user_id)\n",
    "            prediction_data.update(user_data)\n",
    "        \n",
    "        # Generate prediction\n",
    "        prediction_result = prediction_service.predict_fertility(prediction_data)\n",
    "        \n",
    "        return prediction_result\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fertility prediction error: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@api_router.post(\"/predictions/ovulation\", response_model=OvulationPredictionResponse)\n",
    "async def predict_ovulation(\n",
    "    request: OvulationPredictionRequest,\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \"\"\"Predict ovulation day\"\"\"\n",
    "    try:\n",
    "        # Prepare input data for prediction\n",
    "        prediction_data = request.dict()\n",
    "        \n",
    "        # If user_id is provided, fetch additional data from database\n",
    "        if request.user_id:\n",
    "            from app.db.database import fetch_user_data\n",
    "            user_data = await fetch_user_data(request.user_id)\n",
    "            prediction_data.update(user_data)\n",
    "        \n",
    "        # Generate prediction\n",
    "        prediction_result = prediction_service.predict_ovulation(prediction_data)\n",
    "        \n",
    "        return prediction_result\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Ovulation prediction error: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@api_router.post(\"/predictions/feedback\", response_model=PredictionFeedbackResponse)\n",
    "async def submit_feedback(\n",
    "    request: PredictionFeedbackRequest,\n",
    "    background_tasks: BackgroundTasks,\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \"\"\"Submit feedback on a prediction\"\"\"\n",
    "    try:\n",
    "        # Store the feedback in the database\n",
    "        # This would be implemented in a real system\n",
    "        \n",
    "        # Queue model evaluation if sufficient feedback is collected\n",
    "        background_tasks.add_task(\n",
    "            model_trainer.evaluate_model_performance, \n",
    "            user_id=request.user_id if request.user_id else None\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"message\": \"Feedback submitted successfully\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Feedback submission error: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# Model training and management endpoints\n",
    "@api_router.post(\"/models/train\", response_model=TrainingResponse)\n",
    "async def train_model(\n",
    "    request: TrainingRequest,\n",
    "    background_tasks: BackgroundTasks,\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \"\"\"Trigger model training\"\"\"\n",
    "    try:\n",
    "        # Queue the training job\n",
    "        background_tasks.add_task(\n",
    "            model_trainer.train_model,\n",
    "            model_type=request.model_type,\n",
    "            is_global=request.is_global,\n",
    "            user_id=request.user_id if not request.is_global else None\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"job_id\": \"training_job_placeholder\",  # Would be a real job ID in production\n",
    "            \"status\": \"queued\",\n",
    "            \"message\": f\"Training job for {request.model_type} model has been queued\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Model training error: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@api_router.get(\"/models\", response_model=List[ModelInfoResponse])\n",
    "async def get_models(\n",
    "    model_type: Optional[str] = None,\n",
    "    is_global: Optional[bool] = None,\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \"\"\"List available models\"\"\"\n",
    "    try:\n",
    "        # In a real implementation, this would query the database\n",
    "        # For now, return placeholder data\n",
    "        models = [\n",
    "            {\n",
    "                \"id\": 1,\n",
    "                \"name\": \"ARIMA Global Model\",\n",
    "                \"type\": \"arima\",\n",
    "                \"version\": \"v1.0.0\",\n",
    "                \"is_active\": True,\n",
    "                \"is_global\": True,\n",
    "                \"accuracy\": 0.87,\n",
    "                \"trained_at\": \"2023-03-15T12:00:00Z\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": 2,\n",
    "                \"name\": \"Random Forest Global Model\",\n",
    "                \"type\": \"random_forest\",\n",
    "                \"version\": \"v1.0.0\",\n",
    "                \"is_active\": True,\n",
    "                \"is_global\": True,\n",
    "                \"accuracy\": 0.85,\n",
    "                \"trained_at\": \"2023-03-15T12:00:00Z\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": 3,\n",
    "                \"name\": \"Gradient Boosting Global Model\",\n",
    "                \"type\": \"gradient_boosting\",\n",
    "                \"version\": \"v1.0.0\",\n",
    "                \"is_active\": True,\n",
    "                \"is_global\": True,\n",
    "                \"accuracy\": 0.82,\n",
    "                \"trained_at\": \"2023-03-15T12:00:00Z\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Filter by model_type if provided\n",
    "        if model_type:\n",
    "            models = [model for model in models if model[\"type\"] == model_type]\n",
    "        \n",
    "        # Filter by is_global if provided\n",
    "        if is_global is not None:\n",
    "            models = [model for model in models if model[\"is_global\"] == is_global]\n",
    "        \n",
    "        return models\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting models: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@api_router.get(\"/models/{model_id}/metrics\", response_model=ModelMetricsResponse)\n",
    "async def get_model_metrics(\n",
    "    model_id: int,\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \"\"\"Get metrics for a specific model\"\"\"\n",
    "    try:\n",
    "        # In a real implementation, this would query the database\n",
    "        # For now, return placeholder data based on model_id\n",
    "        metrics = {\n",
    "            \"mae\": 1.2,\n",
    "            \"rmse\": 1.8,\n",
    "            \"accuracy\": 0.87,\n",
    "            \"sample_size\": 1000,\n",
    "            \"feature_importance\": {\n",
    "                \"cycle_length\": 0.7,\n",
    "                \"symptoms_pms\": 0.15,\n",
    "                \"stress_level\": 0.1,\n",
    "                \"sleep_quality\": 0.05\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting model metrics: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@api_router.put(\"/models/{model_id}/activate\")\n",
    "async def activate_model(\n",
    "    model_id: int,\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \"\"\"Activate a specific model version\"\"\"\n",
    "    try:\n",
    "        # In a real implementation, this would update the database\n",
    "        # For now, return placeholder success response\n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"message\": f\"Model {model_id} activated successfully\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error activating model: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\"\"\"\n",
    "\n",
    "# 5. Create Pydantic schemas for API\n",
    "prediction_schemas_content = \"\"\"from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Optional, Any, Union\n",
    "from datetime import date, datetime\n",
    "\n",
    "# Period Prediction\n",
    "class PeriodPredictionRequest(BaseModel):\n",
    "    user_id: Optional[int] = None\n",
    "    last_period_start: Optional[date] = None\n",
    "    last_period_end: Optional[date] = None\n",
    "    average_cycle_length: Optional[float] = None\n",
    "    historical_cycles: Optional[List[Dict[str, Any]]] = None\n",
    "    current_symptoms: Optional[List[Dict[str, Any]]] = None\n",
    "\n",
    "class PeriodPredictionResponse(BaseModel):\n",
    "    next_period_date: date\n",
    "    confidence_score: float = Field(..., ge=0.0, le=1.0)\n",
    "    predicted_duration: int\n",
    "    window_start: date\n",
    "    window_end: date\n",
    "    model_version: str\n",
    "\n",
    "# Fertility Prediction\n",
    "class FertilityPredictionRequest(BaseModel):\n",
    "    user_id: Optional[int] = None\n",
    "    last_period_start: Optional[date] = None\n",
    "    average_cycle_length: Optional[float] = None\n",
    "    historical_cycles: Optional[List[Dict[str, Any]]] = None\n",
    "    basal_temperatures: Optional[List[Dict[str, Any]]] = None\n",
    "    cervical_fluid: Optional[List[Dict[str, Any]]] = None\n",
    "\n",
    "class FertilityPredictionResponse(BaseModel):\n",
    "    fertility_window_start: date\n",
    "    fertility_window_end: date\n",
    "    peak_fertility_date: date\n",
    "    confidence_score: float = Field(..., ge=0.0, le=1.0)\n",
    "    model_version: str\n",
    "\n",
    "# Ovulation Prediction\n",
    "class OvulationPredictionRequest(BaseModel):\n",
    "    user_id: Optional[int] = None\n",
    "    last_period_start: Optional[date] = None\n",
    "    average_cycle_length: Optional[float] = None\n",
    "    historical_cycles: Optional[List[Dict[str, Any]]] = None\n",
    "    basal_temperatures: Optional[List[Dict[str, Any]]] = None\n",
    "    cervical_fluid: Optional[List[Dict[str, Any]]] = None\n",
    "    ovulation_tests: Optional[List[Dict[str, Any]]] = None\n",
    "\n",
    "class OvulationPredictionResponse(BaseModel):\n",
    "    ovulation_date: date\n",
    "    confidence_score: float = Field(..., ge=0.0, le=1.0)\n",
    "    model_version: str\n",
    "\n",
    "# Prediction Feedback\n",
    "class PredictionFeedbackRequest(BaseModel):\n",
    "    prediction_id: Optional[int] = None\n",
    "    user_id: Optional[int] = None\n",
    "    prediction_type: str\n",
    "    predicted_date: date\n",
    "    actual_date: date\n",
    "    accurate: bool\n",
    "    error_days: int = Field(0, ge=0)\n",
    "    notes: Optional[str] = None\n",
    "\n",
    "class PredictionFeedbackResponse(BaseModel):\n",
    "    success: bool\n",
    "    message: str\n",
    "\"\"\"\n",
    "\n",
    "training_schemas_content = \"\"\"from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Optional, Any\n",
    "from datetime import datetime\n",
    "\n",
    "class TrainingRequest(BaseModel):\n",
    "    model_type: str = Field(..., description=\"Type of model to train (arima, random_forest, gradient_boosting, ensemble)\")\n",
    "    is_global: bool = Field(..., description=\"Whether to train global model or user-specific model\")\n",
    "    user_id: Optional[int] = Field(None, description=\"User ID for user-specific model training\")\n",
    "    force_retrain: bool = Field(False, description=\"Force retraining even if performance threshold is not met\")\n",
    "\n",
    "class TrainingResponse(BaseModel):\n",
    "    job_id: str\n",
    "    status: str\n",
    "    message: str\n",
    "\n",
    "class ModelInfoResponse(BaseModel):\n",
    "    id: int\n",
    "    name: str\n",
    "    type: str\n",
    "    version: str\n",
    "    is_active: bool\n",
    "    is_global: bool\n",
    "    accuracy: float\n",
    "    trained_at: datetime\n",
    "\n",
    "class ModelMetricsResponse(BaseModel):\n",
    "    mae: float\n",
    "    rmse: float\n",
    "    accuracy: float\n",
    "    sample_size: int\n",
    "    feature_importance: Dict[str, float]\n",
    "\"\"\"\n",
    "\n",
    "# Create __init__.py files for all modules to make them importable\n",
    "def create_init_files(root_dir):\n",
    "    \"\"\"Create __init__.py files in all subdirectories\"\"\"\n",
    "    for path in root_dir.glob('**/*'):\n",
    "        if path.is_dir() and not path.name.startswith('.'):\n",
    "            init_file = path / \"__init__.py\"\n",
    "            if not init_file.exists():\n",
    "                with open(init_file, \"w\") as f:\n",
    "                    f.write(\"# Initialize module\\n\")\n",
    "\n",
    "# Write the files to the directory structure\n",
    "os.makedirs(app_dir / \"core\", exist_ok=True)\n",
    "os.makedirs(app_dir / \"api\" / \"v1\", exist_ok=True)\n",
    "os.makedirs(app_dir / \"db\", exist_ok=True)\n",
    "os.makedirs(app_dir / \"schemas\", exist_ok=True)\n",
    "\n",
    "# Write main.py file\n",
    "with open(ml_engine_dir / \"main.py\", \"w\") as f:\n",
    "    f.write(main_content)\n",
    "\n",
    "# Write config.py file\n",
    "with open(app_dir / \"core\" / \"config.py\", \"w\") as f:\n",
    "    f.write(config_content)\n",
    "\n",
    "# Write database.py file\n",
    "with open(app_dir / \"db\" / \"database.py\", \"w\") as f:\n",
    "    f.write(db_content)\n",
    "\n",
    "# Write API router file\n",
    "with open(app_dir / \"api\" / \"v1\" / \"router.py\", \"w\") as f:\n",
    "    f.write(api_router_content)\n",
    "\n",
    "# Write schema files\n",
    "with open(app_dir / \"schemas\" / \"prediction.py\", \"w\") as f:\n",
    "    f.write(prediction_schemas_content)\n",
    "\n",
    "with open(app_dir / \"schemas\" / \"training.py\", \"w\") as f:\n",
    "    f.write(training_schemas_content)\n",
    "\n",
    "# Create all necessary __init__ files\n",
    "create_init_files(ml_engine_dir)\n",
    "\n",
    "# Create an api.py file in the api directory\n",
    "with open(app_dir / \"api\" / \"__init__.py\", \"w\") as f:\n",
    "    f.write(\"# API module\\n\")\n",
    "\n",
    "with open(app_dir / \"api\" / \"v1\" / \"__init__.py\", \"w\") as f:\n",
    "    f.write(\"# API v1 module\\n\")\n",
    "\n",
    "# Create requirements.txt file\n",
    "requirements_content = \"\"\"fastapi>=0.68.0\n",
    "uvicorn>=0.15.0\n",
    "pydantic>=1.8.2\n",
    "sqlalchemy>=1.4.23\n",
    "pandas>=1.3.3\n",
    "numpy>=1.21.2\n",
    "scikit-learn>=0.24.2\n",
    "statsmodels>=0.13.0\n",
    "mlflow>=1.20.0\n",
    "python-dotenv>=0.19.0\n",
    "psycopg2-binary>=2.9.1\n",
    "\"\"\"\n",
    "\n",
    "with open(ml_engine_dir / \"requirements.txt\", \"w\") as f:\n",
    "    f.write(requirements_content)\n",
    "\n",
    "print(f\"FastAPI application structure created successfully at {ml_engine_dir}\")\n",
    "print(\"Created the following key files:\")\n",
    "print(f\"1. {ml_engine_dir / 'main.py'} - Main FastAPI application\")\n",
    "print(f\"2. {app_dir / 'core' / 'config.py'} - Configuration settings\")\n",
    "print(f\"3. {app_dir / 'db' / 'database.py'} - Database utilities\")\n",
    "print(f\"4. {app_dir / 'api' / 'v1' / 'router.py'} - API router with endpoints\")\n",
    "print(f\"5. {app_dir / 'schemas' / 'prediction.py' and 'training.py'} - Pydantic schemas\")\n",
    "print(f\"6. {ml_engine_dir / 'requirements.txt'} - Project dependencies\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
